{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import json, re, itertools, random, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Spacy for tokenization\n",
    "import spacy \n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "# Gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Episodes Available:  231\n"
     ]
    }
   ],
   "source": [
    "# read Data\n",
    "with open(\"BBT_episodes.json\",\"r\") as file:\n",
    "    data = json.load(file)\n",
    "  \n",
    "# Before converting dict to list add episode markers and remove unwanted texts\n",
    "removeScripts = [\"Story: Chuck Lorre & Bill Prady\",\"Story: Chuck Lorre\",\"None\",'Credits sequence.', 'Credit sequence.','Teleplay: Robert Cohen & Dave Goetsch']\n",
    "scripts = []\n",
    "for key, value in sorted(data.items()):\n",
    "    script = [\"StartofEpisode:\"] + [str(val).strip() for val in value if str(val).strip() not in removeScripts] + [\"EndofEpisode:\"]\n",
    "    scripts.append(script)\n",
    "\n",
    "# All script as single sentence\n",
    "script = \" \".join(map(str,scripts))\n",
    "\n",
    "# No. of Scripts\n",
    "print(\"No. of Episodes Available: \",len(scripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Episodes Available:  231\n",
      "No. of Dialogues:  50957\n"
     ]
    }
   ],
   "source": [
    "# Keep only dialogues from characters and scene descriptions\n",
    "r = re.compile(\".*:.*\")\n",
    "for i in range(len(scripts)):\n",
    "  scripts[i] = list(filter(r.match, scripts[i]))\n",
    "\n",
    "# No. of Scripts\n",
    "print(\"No. of Episodes Available: \",len(scripts))\n",
    "\n",
    "# No. of dialogues\n",
    "print(\"No. of Dialogues: \", np.sum([len(script) for script in scripts]) -  2 * len(scripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StartofEpisode:',\n",
       " 'Scene: A corridor at a sperm bank.',\n",
       " 'Sheldon: So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.',\n",
       " 'Leonard: Agreed, what’s your point?',\n",
       " 'Sheldon: There’s no point, I just think it’s a good idea for a tee-shirt.',\n",
       " 'Leonard: Excuse me?',\n",
       " 'Receptionist: Hang on.',\n",
       " 'Leonard: One across is Aegean, eight down is Nabakov, twenty-six across is MCM, fourteen down is… move your finger… phylum, which makes fourteen across Port-au-Prince. See, Papa Doc’s capital idea, that’s Port-au-Prince. Haiti.',\n",
       " 'Receptionist: Can I help you?',\n",
       " 'Leonard: Yes. Um, is this the High IQ sperm bank?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek\n",
    "scripts[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first word of the dialogue denotes the person speaking, we will give unique identifier for this\n",
    "# replace sheldon: with sheldonspeaks\n",
    "def speaking(s):\n",
    "    if re.findall(\"^[A-z ]+:\", s):\n",
    "        # Extract name and replace space if its multi word before adding speaks to it\n",
    "        toReplace = str(re.findall(\"^[A-z ]+:\", s)[0])\n",
    "        return s.replace(toReplace, toReplace[:-1].replace(\" \",\"\") + \"speaks\")\n",
    "\n",
    "def recursively_apply(l, f):\n",
    "    for n, i in enumerate(l):\n",
    "        if type(i) is list:\n",
    "            l[n] = recursively_apply(l[n], f)\n",
    "        elif type(i) is str:\n",
    "            l[n] = f(i)\n",
    "    return l\n",
    "  \n",
    "# Apply function\n",
    "modified_scripts = recursively_apply(scripts, speaking)\n",
    "\n",
    "# Flatten list\n",
    "modified_scripts = [str(y) for x in modified_scripts for y in x]\n",
    "\n",
    "# Use only dialogues\n",
    "r = re.compile(\".*speaks.*\")\n",
    "modified_scripts = list(filter(r.match, modified_scripts))\n",
    "\n",
    "# Get starts of episode indexs\n",
    "start_ind = [ind for ind, val in enumerate(modified_scripts) if val == 'StartofEpisodespeaks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StartofEpisodespeaks',\n",
       " 'Scenespeaks The living room of the apartment. Leonard and Sheldon are playing the three dimensional chess game from the original Star Trek series. It is Leonard’s move. He takes his time, moving round the board and checking things from various angles. Finally he tentatively makes a move. Sheldon moves almost immediately.',\n",
       " 'Sheldonspeaks Checkmate.',\n",
       " 'Leonardspeaks O-o-o-o-h! Again?',\n",
       " 'Sheldonspeaks Obviously you’re not well suited for three-dimensional chess, perhaps three dimensional candyland would be more your speed.',\n",
       " 'Leonardspeaks Just reset the board.',\n",
       " 'Sheldonspeaks It must be humbling to suck on so many different levels.',\n",
       " 'Leonardspeaks Hey!',\n",
       " 'Pennyspeaks Did you get my mail.',\n",
       " 'Leonardspeaks Yeah, right here. How was Nebraska?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random check\n",
    "ind = start_ind[10]\n",
    "modified_scripts[ind: ind+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect NER types so that similar meaning multi-words can be standardised\n",
    "NER = {}\n",
    "for doc in nlp.pipe(modified_scripts, n_threads = -1, batch_size= 5000):\n",
    "    for w in doc.ents:\n",
    "        if w.label_ in NER.keys():\n",
    "            if not bool(re.search('speaks', str(w))):\n",
    "                NER[w.label_].append(str(w))\n",
    "        else:\n",
    "            NER[w.label_] = []\n",
    "            if not bool(re.search('speaks', str(w))):\n",
    "                NER[w.label_].append(str(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Nr of occurence of each NER type\n",
    "def stat(NER_LIST):\n",
    "    stat = pd.DataFrame(pd.DataFrame(NER_LIST)[0].value_counts())\n",
    "    stat.reset_index(inplace = True)\n",
    "    stat.columns = [\"Name\", \"Count\"]\n",
    "    return stat.loc[stat.Count > 10]\n",
    "\n",
    "NER_dflist = {}\n",
    "for ner in list(NER.keys()):\n",
    "    NER_dflist[ner] = stat(NER[ner])\n",
    "\n",
    "# Write to file\n",
    "def save_xls(dict_df, path):\n",
    "    writer = pd.ExcelWriter(path)\n",
    "    for key in dict_df:\n",
    "        dict_df[key].to_excel(writer, '%s' % key, index = False)\n",
    "    writer.save()\n",
    "        \n",
    "#save_xls(NER_dflist, 'NER_stat.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sheldonspeaks</td>\n",
       "      <td>10935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Leonardspeaks</td>\n",
       "      <td>9242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pennyspeaks</td>\n",
       "      <td>7246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Howardspeaks</td>\n",
       "      <td>5555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rajspeaks</td>\n",
       "      <td>4462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amyspeaks</td>\n",
       "      <td>3350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernadettespeaks</td>\n",
       "      <td>2599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Scenespeaks</td>\n",
       "      <td>2115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stuartspeaks</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EndofEpisodespeaks</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name  Count\n",
       "0       Sheldonspeaks  10935\n",
       "1       Leonardspeaks   9242\n",
       "2         Pennyspeaks   7246\n",
       "3        Howardspeaks   5555\n",
       "4           Rajspeaks   4462\n",
       "5           Amyspeaks   3350\n",
       "6    Bernadettespeaks   2599\n",
       "7         Scenespeaks   2115\n",
       "8        Stuartspeaks    716\n",
       "9  EndofEpisodespeaks    231"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of dialogues spoken by each person\n",
    "regex = r'\\b\\w+\\b'\n",
    "speaker = [x for x in re.findall(regex,\" \".join(modified_scripts)) if bool(re.search(\"speaks\", x))]\n",
    "speaker = stat(speaker)\n",
    "speaker.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace every item in list\n",
    "replace = [ (['Sheldon Cooper', 'Shelly'], 'Sheldon'),\n",
    "            (['Wil Wheaton'], 'Wheaton'),\n",
    "            (['Stephen Hawking'], 'Hawking'),\n",
    "            (['Howie', 'Howard Wolowitz', 'Wolowitz'], 'Howard'),\n",
    "            (['Amy Farrah Fowler'], 'Amy'),\n",
    "            (['Bernie'], 'Bernadette'),\n",
    "            (['Hofstadter', 'Leonard Hofstadter'], 'Leonard'),\n",
    "            (['Rajesh', 'Koothrappali'], 'Raj'),\n",
    "            (['Leslie Winkle'], 'Leslie'),\n",
    "            (['The Cheesecake Factory', 'the Cheesecake Factory'], 'CheesecakeFactory'),\n",
    "            (['Game of Thrones'], 'GameofThrones'),\n",
    "            (['Fun with Flags'], 'FunwithFlags'),\n",
    "            (['Los Angeles'], 'LosAngeles'),\n",
    "            (['New Jersey'], 'NewJersey'),\n",
    "            (['New Delhi'], 'NewDelhi'),\n",
    "            (['Star Trek'], 'StarTrek'),\n",
    "            (['Star Wars'], 'StarWars')\n",
    "          ]\n",
    "\n",
    "for val in replace:\n",
    "    toreplace = \"(\" + \")|(\".join(val[0]) + \")\"\n",
    "    for i in range(len(modified_scripts)):\n",
    "        modified_scripts[i] = re.sub(toreplace, val[1], modified_scripts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Check if properly replaced or not\n",
    "r = re.compile(\".*Shelly.*\")\n",
    "list(filter(r.match, modified_scripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the script into normalised words\n",
    "scripts_norms = []\n",
    "scripts_tokens = []\n",
    "for doc in nlp.pipe(modified_scripts, n_threads = -1, batch_size= 5000):\n",
    "    norms = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.text.lower() for word in doc ]\n",
    "    tokens = [word.norm_.lower().strip() if word.norm_ != 'gonna' else 'a' for word in doc ]\n",
    "    scripts_norms.append(norms)\n",
    "    scripts_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pennyspeaks leo , you are a very sweet , really funny guy . you are going to do okay .\n",
      "\n",
      "tobyspeaks one day at a time , penny , one day at a time .\n",
      "\n",
      "leonardspeaks how long is he going to stay here .\n",
      "\n",
      "sheldonspeaks he 's a homeless drug addict , leonard , where is he going to go ? boy , you have a lot to learn about lying .\n",
      "\n",
      "endofepisodespeaks\n",
      "\n",
      "startofepisodespeaks\n",
      "\n",
      "scenespeaks the living room of the apartment . leonard and sheldon are playing the three dimensional chess game from the original startrek series . it is leonard 's move . he takes his time , moving round the board and checking things from various angles . finally he tentatively makes a move . sheldon moves almost immediately .\n",
      "\n",
      "sheldonspeaks checkmate .\n",
      "\n",
      "leonardspeaks o - o - o - o - h ! again ?\n",
      "\n",
      "sheldonspeaks obviously you are not well suited for three - dimensional chess , perhaps three dimensional candyland would be more your speed .\n"
     ]
    }
   ],
   "source": [
    "# print list of scripts\n",
    "def prnt_tok(lst):\n",
    "    print( \"\\n\\n\".join([\" \".join(l) for l in lst] ))\n",
    "\n",
    "ind = start_ind[10]\n",
    "prnt_tok(scripts_tokens[ind - 5: ind+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a unique index to them.\n",
    "START, END = '<s>', '</s>'\n",
    "START_EP, END_EP = 'startofepisodespeaks', 'endofepisodespeaks'\n",
    "\n",
    "# Add next person to speak as last word of the script, along with start and end identifiers\n",
    "for i in range(len(scripts_tokens) - 1):\n",
    "  if scripts_tokens[i] == [END_EP] or scripts_tokens[i + 1] == [END_EP] or scripts_tokens[i] == [START_EP]:\n",
    "    scripts_tokens[i] = [START] + scripts_tokens[i] + [END]\n",
    "  else:\n",
    "    scripts_tokens[i] = [START] + scripts_tokens[i] + [scripts_tokens[i + 1][0]] + [END]\n",
    "    \n",
    "# Remove the last element\n",
    "del scripts_tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> scenespeaks the climbing centre . howardspeaks </s>\n",
      "\n",
      "<s> howardspeaks you got to give him credit for sticking with it . leonardspeaks </s>\n",
      "\n",
      "<s> leonardspeaks i do not think he have it in him . rajspeaks </s>\n",
      "\n",
      "<s> rajspeaks he almost made it to the top this time . </s>\n",
      "\n",
      "<s> endofepisodespeaks </s>\n",
      "\n",
      "<s> startofepisodespeaks </s>\n",
      "\n",
      "<s> scenespeaks the apartment . the guys are studying a complex chart on the whiteboard . leonardspeaks </s>\n",
      "\n",
      "<s> leonardspeaks hmmm . sheldonspeaks </s>\n",
      "\n",
      "<s> sheldonspeaks the problem appears to be unsolvable . rajspeaks </s>\n",
      "\n",
      "<s> rajspeaks maybe you could run some computer simulations . howardspeaks </s>\n"
     ]
    }
   ],
   "source": [
    "# random check\n",
    "ind = start_ind[30]\n",
    "prnt_tok(scripts_tokens[ind - 5 : ind + 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scenespeaks The climbing centre.',\n",
       " 'Howardspeaks You gotta give him credit for sticking with it.',\n",
       " 'Leonardspeaks I didn’t think he had it in him.',\n",
       " 'Rajspeaks He almost made it to the top this time.',\n",
       " 'EndofEpisodespeaks',\n",
       " 'StartofEpisodespeaks',\n",
       " 'Scenespeaks The apartment. The guys are studying a complex chart on the whiteboard.',\n",
       " 'Leonardspeaks Hmmm.',\n",
       " 'Sheldonspeaks The problem appears to be unsolvable.',\n",
       " 'Rajspeaks Maybe you could run some computer simulations.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_scripts[ind - 5: ind + 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ramonaspeaks mmm . no big deal , i enjoy spending time with you . sheldonspeaks </s>\n",
      "\n",
      "<s> sheldonspeaks and i with you . question , are you seeking a romantic relationship with me ? ramonaspeaks </s>\n",
      "\n",
      "<s> ramonaspeaks what if i were ? sheldonspeaks </s>\n",
      "\n",
      "<s> sheldonspeaks well , that would raise a number of problems . we are colleagues . i am currently in a relation … excuse me a moment . scenespeaks </s>\n",
      "\n",
      "<s> scenespeaks princeton . </s>\n"
     ]
    }
   ],
   "source": [
    "# Check if last episode is mapped correctly\n",
    "prnt_tok(scripts_tokens[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> pennyspeaks apples and oranges here , sheldon . i am telling you , that girl is a user , iceskating through the life on her looks , taking advantage of innocent weak - willed men , getting auditions for stupid network shows . it creams my corn . sheldonspeaks </s>\n",
      "\n",
      "<s> sheldonspeaks may i interject something here ? pennyspeaks </s>\n",
      "\n",
      "<s> pennyspeaks please . sheldonspeaks </s>\n",
      "\n",
      "<s> sheldonspeaks you got the wrong mustard . scenespeaks </s>\n",
      "\n",
      "<s> scenespeaks the laundry room . aliciaspeaks </s>\n",
      "\n",
      "<s> aliciaspeaks guess what ? i got the part on csi . pennyspeaks </s>\n",
      "\n",
      "<s> pennyspeaks oh boy . aliciaspeaks </s>\n",
      "\n",
      "<s> aliciaspeaks something wrong ? pennyspeaks </s>\n",
      "\n",
      "<s> pennyspeaks uh , no . no , no , no , you know , congratulations , i think you will make a great hooker . aliciaspeaks </s>\n",
      "\n",
      "<s> aliciaspeaks thank you . hey , i got to ask you something , how much do physicists make ? pennyspeaks </s>\n"
     ]
    }
   ],
   "source": [
    "# Find weird scripts: no dialogue at all\n",
    "find = ['<s>', 'pennyspeaks', 'aliciaspeaks', '</s>']\n",
    "ind = []\n",
    "for i, v in enumerate(scripts_tokens):\n",
    "    if v == find:\n",
    "        ind.append(i)\n",
    "\n",
    "# Delete the original script\n",
    "del scripts_tokens[ind[0]]\n",
    "\n",
    "# Fix continuity\n",
    "scripts_tokens[ind[0] - 1][-2] = scripts_tokens[ind[0]][1]\n",
    "prnt_tok(scripts_tokens[ind[0] - 5 : ind[0] + 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>3226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>3123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>2685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>2727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>2659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>2594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>2418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>2118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>2027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>1846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18</td>\n",
       "      <td>1653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>1491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21</td>\n",
       "      <td>1365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22</td>\n",
       "      <td>1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>23</td>\n",
       "      <td>1061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24</td>\n",
       "      <td>990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>26</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>27</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>28</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>30</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>31</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>33</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>34</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>35</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>36</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>37</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>38</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>39</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>40</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>41</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>42</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>43</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>44</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>45</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>46</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>47</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>48</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>49</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>51</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>52</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>53</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>54</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>55</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>56</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>58</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>59</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>60</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>61</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>62</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>63</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>64</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Length  Count\n",
       "0        5     38\n",
       "1        6   3226\n",
       "2        7   2265\n",
       "3        8   3123\n",
       "4        9   2685\n",
       "5       10   3071\n",
       "6       11   2727\n",
       "7       12   2659\n",
       "8       13   2594\n",
       "9       14   2418\n",
       "10      15   2118\n",
       "11      16   2027\n",
       "12      17   1846\n",
       "13      18   1653\n",
       "14      19   1510\n",
       "15      20   1491\n",
       "16      21   1365\n",
       "17      22   1169\n",
       "18      23   1061\n",
       "19      24    990\n",
       "20      25    892\n",
       "21      26    810\n",
       "22      27    757\n",
       "23      28    717\n",
       "24      29    610\n",
       "25      30    569\n",
       "26      31    534\n",
       "27      32    495\n",
       "28      33    424\n",
       "29      34    407\n",
       "30      35    356\n",
       "31      36    350\n",
       "32      37    284\n",
       "33      38    295\n",
       "34      39    276\n",
       "35      40    234\n",
       "36      41    215\n",
       "37      42    224\n",
       "38      43    167\n",
       "39      44    152\n",
       "40      45    156\n",
       "41      46    123\n",
       "42      47    134\n",
       "43      48     95\n",
       "44      49    128\n",
       "45      50    100\n",
       "46      51    100\n",
       "47      52     72\n",
       "48      53     67\n",
       "49      54     74\n",
       "50      55     72\n",
       "51      56     67\n",
       "52      57     57\n",
       "53      58     39\n",
       "54      59     56\n",
       "55      60     48\n",
       "56      61     33\n",
       "57      62     33\n",
       "58      63     40\n",
       "59      64     41"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of words in each dialogue\n",
    "First, Last = [START, START_EP, END], [START, END_EP, END]\n",
    "lens = [len(script) for script in scripts_tokens if script != First and script != Last]\n",
    "length, count = np.unique(lens, return_counts= True)\n",
    "pd.DataFrame({\"Length\" : length, \"Count\" : count}).head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'leonardspeaks', 'morning', 'sheldonspeaks', '</s>'],\n",
       " ['<s>', 'scenespeaks', 'cheesecakefactory', 'sheldonspeaks', '</s>'],\n",
       " ['<s>', 'togetherspeaks', 'awesome', '!', '</s>'],\n",
       " ['<s>', 'scenespeaks', 'cheesecakefactory', 'pennyspeaks', '</s>'],\n",
       " ['<s>', 'leonardspeaks', 'yep', 'scenespeaks', '</s>'],\n",
       " ['<s>', 'ramonaspeaks', '4-a.', 'pennyspeaks', '</s>'],\n",
       " ['<s>', 'computervoicespeaks', 'honey', '.', '</s>'],\n",
       " ['<s>', 'leonardspeaks', 'great', '.', '</s>'],\n",
       " ['<s>', 'pennyspeaks', 'yes', 'sheldonspeaks', '</s>'],\n",
       " ['<s>', 'leonardspeaks', 'yes', 'howardspeaks', '</s>']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all dialogues with length of 5\n",
    "length = 5\n",
    "len_fil = [s for s in scripts_tokens if len(s) == length]\n",
    "len_fil[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert consecutive dialogues as input and target for the model \n",
    "# Remove the next person to speak from target dialogue\n",
    "input = []\n",
    "output = []\n",
    "\n",
    "for i in range(len(scripts_tokens) - 1):\n",
    "  if scripts_tokens[i + 1] == Last or scripts_tokens[i] == First or scripts_tokens[i] == Last:\n",
    "    pass\n",
    "  else:\n",
    "    input.append(scripts_tokens[i])\n",
    "    output.append(scripts_tokens[i + 1][:-2] + [END])\n",
    "    \n",
    "# Now, make them as `script_pairs`. \n",
    "script_pairs = list(zip(input, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> sheldonspeaks how thoughtful . thank you . ramonaspeaks </s>\n",
      "<s> ramonaspeaks mmm . no big deal , i enjoy spending time with you . </s>\n",
      "\n",
      "<s> ramonaspeaks mmm . no big deal , i enjoy spending time with you . sheldonspeaks </s>\n",
      "<s> sheldonspeaks and i with you . question , are you seeking a romantic relationship with me ? </s>\n",
      "\n",
      "<s> sheldonspeaks and i with you . question , are you seeking a romantic relationship with me ? ramonaspeaks </s>\n",
      "<s> ramonaspeaks what if i were ? </s>\n",
      "\n",
      "<s> ramonaspeaks what if i were ? sheldonspeaks </s>\n",
      "<s> sheldonspeaks well , that would raise a number of problems . we are colleagues . i am currently in a relation … excuse me a moment . </s>\n",
      "\n",
      "<s> sheldonspeaks well , that would raise a number of problems . we are colleagues . i am currently in a relation … excuse me a moment . scenespeaks </s>\n",
      "<s> scenespeaks princeton </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print functions and random checks\n",
    "def prnt_pair(tup):\n",
    "    inp, out = tup\n",
    "    prn = \" \".join(inp) + \"\\n\" + \" \".join(out) \n",
    "    print(prn.strip(), end =\"\\n\\n\")\n",
    "\n",
    "def prnt_scripts(scripts):\n",
    "    [prnt_pair(pair) for pair in scripts]\n",
    "    \n",
    "# Sample of input and target for the model\n",
    "prnt_scripts(script_pairs[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of scripts:  50548\n",
      "No. of scripts after filtering:  39006\n"
     ]
    }
   ],
   "source": [
    "# Decide maxmium sequence length\n",
    "print(\"No. of scripts before trimming: \", len(script_pairs))\n",
    "max_len = 30\n",
    "trim_scripts = [pair for pair in script_pairs if len(pair[0]) <= max_len and len(pair[1]) <= max_len]\n",
    "\n",
    "print(\"No. of scripts after filtering: \", len(trim_scripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> sheldonspeaks how thoughtful . thank you . ramonaspeaks </s>\n",
      "<s> ramonaspeaks mmm . no big deal , i enjoy spending time with you . </s>\n",
      "\n",
      "<s> ramonaspeaks mmm . no big deal , i enjoy spending time with you . sheldonspeaks </s>\n",
      "<s> sheldonspeaks and i with you . question , are you seeking a romantic relationship with me ? </s>\n",
      "\n",
      "<s> sheldonspeaks and i with you . question , are you seeking a romantic relationship with me ? ramonaspeaks </s>\n",
      "<s> ramonaspeaks what if i were ? </s>\n",
      "\n",
      "<s> ramonaspeaks what if i were ? sheldonspeaks </s>\n",
      "<s> sheldonspeaks well , that would raise a number of problems . we are colleagues . i am currently in a relation … excuse me a moment . </s>\n",
      "\n",
      "<s> sheldonspeaks well , that would raise a number of problems . we are colleagues . i am currently in a relation … excuse me a moment . scenespeaks </s>\n",
      "<s> scenespeaks princeton </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prnt_scripts(trim_scripts[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48951220, 92598900)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_data = [pair[0] for pair in trim_scripts]\n",
    "emd_model = gensim.models.Word2Vec(emb_data, size = 300, min_count = 5, workers = 3)\n",
    "emd_model.train(scripts_tokens, total_examples=len(emb_data), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only inputs for word embedding model and build vocabulary\n",
    "emb_data = [pair[0] for pair in trim_scripts]\n",
    "emd_model = gensim.models.Word2Vec(size = 300, min_count = 5, iter = 10, workers=4)\n",
    "emd_model.build_vocab(emb_data)\n",
    "\n",
    "# load initial embeddings for common words from google wordvec\n",
    "emd_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True, lockf = 1.0)\n",
    "\n",
    "# train for new words\n",
    "emd_model.train(emb_data, total_examples = len(emb_data), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 70)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Mappings\n",
    "pad, pad_idx = '<pad>', 0\n",
    "unk, unk_idx = '<unk>', 1\n",
    "vocab = [pad, unk, *list(emd_model.wv.vocab.keys())]\n",
    "word2index = {word:emd_model.wv.vocab[word].index + 2 for word in vocab if word in emd_model.wv.vocab}\n",
    "word2index[pad] = pad_idx\n",
    "word2index[unk] = unk_idx\n",
    "index2word = {index:word for word, index in word2index.items()}\n",
    "\n",
    "# Example\n",
    "word2index['leonard'], word2index[index2word[word2index['leonard']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding is zero vector and initialize random vec for unknown token\n",
    "vectors = np.zeros((len(vocab), 300))\n",
    "vectors[1] = np.random.uniform(-0.1, 0.1, (1, 300))\n",
    "for i in np.arange(2, len(vocab)):\n",
    "    vectors[i] = emd_model.wv[index2word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 15066\n"
     ]
    }
   ],
   "source": [
    "# get count of words in training data\n",
    "word_count = {}\n",
    "\n",
    "for sentence in trim_scripts:\n",
    "    for word in sentence[0]:\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    "            \n",
    "print('Number of unique words:', len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "np.save('vectors.npy', vectors)\n",
    "with open('word2index.json', 'w') as fp:\n",
    "    json.dump(word2index, fp)\n",
    "\n",
    "with open('scripts.pickle', 'wb') as f:\n",
    "    pickle.dump(trim_scripts, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2index.json', 'r') as fp:\n",
    "    word2index = json.load(fp)\n",
    "vectors = np.load('vectors.npy')\n",
    "max_len = 30\n",
    "\n",
    "with open('scripts.pickle', 'rb') as f:\n",
    "    trim_scripts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBTDataset(Dataset):\n",
    "    '''\n",
    "        BBT scripts modelling.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, word2idx, seq_length):\n",
    "        self.inp_script = [pair[0] for pair in data]\n",
    "        self.out_script = [pair[1] for pair in data]\n",
    "        self.word2idx = word2idx\n",
    "        self.seq_length = seq_length\n",
    "        self.unk = set()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inp_script)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "            Returns a pair of tensors containing word indices\n",
    "            for the specified sentence pair in the dataset.\n",
    "        '''\n",
    "        \n",
    "        # init torch tensors, note that 0 is the padding index\n",
    "        inp_tensor = torch.zeros(self.seq_length, dtype=torch.long)\n",
    "        out_tensor = torch.zeros(self.seq_length, dtype=torch.long)\n",
    "        \n",
    "        # Get sentence pair\n",
    "        input_script = self.inp_script[idx]\n",
    "        output_script = self.out_script[idx]\n",
    "        \n",
    "        \n",
    "        # Load word indices\n",
    "        for i, word in enumerate(input_script):\n",
    "            if word in self.word2idx and word_count[word] > 5:\n",
    "                inp_tensor[i] = self.word2idx[word]\n",
    "            else:\n",
    "                inp_tensor[i] = self.word2idx[unk]\n",
    "                self.unk.add(word)\n",
    "        \n",
    "        for i, word in enumerate(output_script):\n",
    "            if word in self.word2idx and word_count[word] > 5:\n",
    "                out_tensor[i] = self.word2idx[word]\n",
    "            else:\n",
    "                out_tensor[i] = self.word2idx[unk]\n",
    "                self.unk.add(word)\n",
    "            \n",
    "        sample = {'input_tensor': inp_tensor, 'input_script': input_script,\n",
    "                  'output_tensor': out_tensor, 'output_script': output_script}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbtDataset = BBTDataset(trim_scripts, word2index, seq_length = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_tensor', 'input_script', 'output_tensor', 'output_script'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = bbtDataset[20]\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input example:\n",
      "Sentence: ['<s>', 'leonardspeaks', 'see', 'you', '.', 'scenespeaks', '</s>']\n",
      "Tensor: tensor([  3,   7,  91,   8,   2,  28,   4,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0])\n",
      "\n",
      "Target example:\n",
      "Sentence: ['<s>', 'scenespeaks', 'the', 'stairs', 'of', 'the', 'apartment', 'building', '.', '</s>']\n",
      "Tensor: tensor([   3,   28,   13,  877,   29,   13,   95,  522,    2,    4,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "print('Input example:')\n",
    "print('Sentence:', sample['input_script'])\n",
    "print('Tensor:', sample['input_tensor'])\n",
    "\n",
    "print('\\nTarget example:')\n",
    "print('Sentence:', sample['output_script'])\n",
    "print('Tensor:', sample['output_tensor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader to check how the batching works\n",
    "dataloader = DataLoader(bbtDataset, batch_size=5,shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Script: ('<s>', '<s>', '<s>', '<s>', '<s>')\n",
      "Output Script: ('<s>', '<s>', '<s>', '<s>', '<s>') \n",
      "\n",
      "Input Script: ('leonardspeaks', 'sheldonspeaks', 'leonardspeaks', 'receptionistspeaks', 'leonardspeaks')\n",
      "Output Script: ('sheldonspeaks', 'leonardspeaks', 'receptionistspeaks', 'leonardspeaks', 'receptionistspeaks') \n",
      "\n",
      "Input Script: ('agreed', 'there', 'excuse', 'can', 'yes')\n",
      "Output Script: ('there', 'excuse', 'hang', 'yes', 'if') \n",
      "\n",
      "Input Script: (',', \"'s\", 'me', 'i', '.')\n",
      "Output Script: (\"'s\", 'me', 'on', '.', 'you') \n",
      "\n",
      "Input Script: ('what', 'no', '?', 'help', 'um')\n",
      "Output Script: ('no', '?', '.', 'um', 'have') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    batch = i\n",
    "    break\n",
    "\n",
    "for i in range(5):\n",
    "    print('Input Script:', batch['input_script'][i])\n",
    "    print('Output Script:', batch['output_script'][i],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBiGRU(nn.Module):\n",
    "    def __init__(self, hidden_size, pretrained_embeddings):\n",
    "        super(EncoderBiGRU, self).__init__()\n",
    "        \n",
    "        # Model parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = pretrained_embeddings.shape[1]\n",
    "        self.vocab_size = pretrained_embeddings.shape[0]\n",
    "        self.num_layers = 2\n",
    "        self.dropout = 0.1 if self.num_layers > 1 else 0\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        \n",
    "        # Construct the layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.gru = nn.GRU(self.embedding_dim,\n",
    "                            self.hidden_size,\n",
    "                            self.num_layers,\n",
    "                            batch_first = True,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional)\n",
    "        \n",
    "        # Initialize hidden to hidden weights in GRU to the Identity matrix\n",
    "        # PyTorch GRU has 3 different hidden to hidden weights stacked in one matrix\n",
    "        identity_init = torch.eye(self.hidden_size)\n",
    "        self.gru.weight_hh_l0.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "        self.gru.weight_hh_l0_reverse.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "        self.gru.weight_hh_l1.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "        self.gru.weight_hh_l1_reverse.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = self.gru(embedded, hidden)\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        \n",
    "        hidden_state = torch.zeros(self.num_layers*(2 if self.bidirectional else 1),\n",
    "                                   batch_size,\n",
    "                                   self.hidden_size, \n",
    "                                   device=device)\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final output of the GRU Encoder on our test input is: \n",
      "\n",
      " torch.Size([1, 3, 10])\n",
      "\n",
      "\n",
      "Encoder output tensor: \n",
      "\n",
      " tensor([[[-0.0499, -0.1631,  0.2637,  0.3069,  0.0464, -0.1635, -0.4241,\n",
      "           0.1198, -0.3655, -0.2922],\n",
      "         [ 0.2388, -0.3575,  0.5471,  0.1993,  0.1039, -0.0111, -0.6888,\n",
      "          -0.0569, -0.3468, -0.3591],\n",
      "         [ 0.1542, -0.3440,  0.6296,  0.3255,  0.1205, -0.0647, -0.2563,\n",
      "           0.0969, -0.1122, -0.0884]]])\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 1\n",
    "test_seq_length = 3\n",
    "test_hidden_size = 5\n",
    "test_encoder = EncoderBiGRU(test_hidden_size, vectors).to(device)\n",
    "test_hidden = test_encoder.initHidden(test_batch_size)\n",
    "\n",
    "# Create an input tensor of random indices\n",
    "test_inputs = torch.randint(0, 50, (test_batch_size, test_seq_length), dtype=torch.long, device=device)\n",
    "\n",
    "test_encoder_output, test_encoder_hidden = test_encoder.forward(test_inputs, test_hidden)\n",
    "\n",
    "print(\"The final output of the GRU Encoder on our test input is: \\n\\n\", test_encoder_output.shape)\n",
    "\n",
    "print('\\n\\nEncoder output tensor: \\n\\n', test_encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7023, -0.3842, -0.0874, -0.9902, -0.2901]],\n",
       "\n",
       "        [[-0.1193,  0.1815,  0.9926,  0.0030, -0.5165]],\n",
       "\n",
       "        [[ 0.1542, -0.3440,  0.6296,  0.3255,  0.1205]],\n",
       "\n",
       "        [[-0.1635, -0.4241,  0.1198, -0.3655, -0.2922]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderGRU(nn.Module):\n",
    "    def __init__(self, decoder_hidden_size, pretrained_embeddings, seq_length):\n",
    "        super(AttnDecoderGRU, self).__init__()\n",
    "        # Embedding parameters\n",
    "        self.embedding_dim = pretrained_embeddings.shape[1]\n",
    "        self.output_vocab_size = pretrained_embeddings.shape[0]\n",
    "        \n",
    "        # GRU parameters\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.num_layers = 2 # Potentially add more layers to LSTM later\n",
    "        self.dropout = 0.1 if self.num_layers > 1 else 0 # Potentially add dropout later\n",
    "        \n",
    "        # Attention parameters\n",
    "        self.seq_length = max_len\n",
    "        self.encoder_hidden_dim = 2*decoder_hidden_size\n",
    "        \n",
    "        # Construct embedding layer for output \n",
    "        self.embedding = nn.Embedding(self.output_vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.embedding.weight.requires_grad = False # we don't want to train the embedding weights\n",
    "        \n",
    "        # Construct layer that calculates attentional weights\n",
    "        self.attn = nn.Linear(self.decoder_hidden_size + self.embedding_dim, self.seq_length)\n",
    "        \n",
    "        # Construct layer that compresses the combined matrix of the input embeddings\n",
    "        # and the encoder inputs after attention has been applied\n",
    "        self.attn_with_input = nn.Linear(self.embedding_dim + self.encoder_hidden_dim, self.embedding_dim)\n",
    "        \n",
    "        # gru for Decoder\n",
    "        self.gru = nn.GRU(self.embedding_dim,\n",
    "                            self.decoder_hidden_size,\n",
    "                            self.num_layers,\n",
    "                            dropout=self.dropout)\n",
    "        \n",
    "        # Initialize hidden to hidden weights in GRU to the Identity matrix\n",
    "        # PyTorch GRU has 3 different hidden to hidden weights stacked in one matrix\n",
    "        identity_init = torch.eye(self.decoder_hidden_size)\n",
    "        self.gru.weight_hh_l0.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "        self.gru.weight_hh_l1.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(self.decoder_hidden_size, self.output_vocab_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_output):\n",
    "        # Input word indices, should have dim(1, batch_size), output will be (1, batch_size, embedding_dim)\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # Calculate Attention weights\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((hidden[0], embedded[0]), 1)), dim=1)\n",
    "        attn_weights = attn_weights.unsqueeze(1) # Add dimension for batch matrix multiplication\n",
    "        \n",
    "        # Apply Attention weights\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_output)\n",
    "        attn_applied = attn_applied.squeeze(1) # Remove extra dimension, dim are now (batch_size, encoder_hidden_size)\n",
    "        \n",
    "        # Prepare GRU input tensor\n",
    "\n",
    "        attn_combined = torch.cat((embedded[0], attn_applied), 1) # Combine embedding input and attn_applied,\n",
    "        gru_input = F.relu(self.attn_with_input(attn_combined)) # pass through fully connected with ReLU\n",
    "        gru_input = gru_input.unsqueeze(0) # Add seq dimension so tensor has expected dimensions for lstm\n",
    "        \n",
    "        output, hidden = self.gru(gru_input, hidden) # Output dim = (1, batch_size, decoder_hidden_size)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1) # softmax over all words in vocab\n",
    "        \n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the decoder on sample inputs to check that the dimensions of everything is correct\n",
    "test_decoder_hidden_size = 5\n",
    "\n",
    "test_decoder = AttnDecoderGRU(test_decoder_hidden_size, vectors, test_seq_length).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    # Initialize encoder hidden state\n",
    "    encoder_hidden = encoder.initHidden(input_tensor.shape[0])\n",
    "    \n",
    "    # clear the gradients in the optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # run forward pass through encoder on entire sequence\n",
    "    encoder_output, encoder_hidden = encoder.forward(input_tensor, encoder_hidden)\n",
    "    \n",
    "    # Initialize decoder input(Start of Sentence tag) and hidden state from encoder\n",
    "    decoder_input =  torch.tensor([word2index[START]]*input_tensor.shape[0], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Use correct initial hidden state dimensions depending on type of RNN\n",
    "    decoder_hidden = encoder_hidden[1::2].contiguous()\n",
    "    \n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    \n",
    "    # Implement teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Step through target output sequence\n",
    "        for di in range(1, max_len):\n",
    "            output, decoder_hidden, attn_weights = decoder(decoder_input,\n",
    "                                                           decoder_hidden,\n",
    "                                                           encoder_output)\n",
    "            \n",
    "            # Feed target as input to next item in the sequence\n",
    "            decoder_input = target_tensor[di].unsqueeze(0)\n",
    "            loss += criterion(output, target_tensor[di])\n",
    "    else:\n",
    "        # Step through target output sequence\n",
    "        for di in range(1, max_len):\n",
    "            \n",
    "            # Forward pass through decoder\n",
    "            output, decoder_hidden, attn_weights = decoder(decoder_input,\n",
    "                                                           decoder_hidden,\n",
    "                                                           encoder_output)\n",
    "            \n",
    "            # Feed output as input to next item in the sequence\n",
    "            decoder_input = output.topk(1)[1].view(1,-1).detach()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss += criterion(output, target_tensor[di])\n",
    "    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip the gradients\n",
    "    nn.utils.clip_grad_norm_(encoder.parameters(), 25)\n",
    "    nn.utils.clip_grad_norm_(decoder.parameters(), 25)\n",
    "    \n",
    "    # Update the weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, dataloader, epochs, print_every_n_batches=100, learning_rate=0.01):\n",
    "    \n",
    "    # keep track of losses\n",
    "    plot_losses = []\n",
    "\n",
    "    # Initialize Encoder Optimizer\n",
    "    encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "    encoder_optimizer = optim.Adam(encoder_parameters, lr=learning_rate)\n",
    "    \n",
    "    # Initialize Decoder Optimizer\n",
    "    decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "    decoder_optimizer = optim.Adam(decoder_parameters, lr=learning_rate)\n",
    "\n",
    "    # Specify loss function, ignore the <pad> token index so it does not contribute to loss.\n",
    "    criterion = nn.NLLLoss(ignore_index=0)\n",
    "    \n",
    "    # Cycle through epochs\n",
    "    for epoch in range(epochs):\n",
    "        loss_avg = 0\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        # Cycle through batches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            \n",
    "            input_tensor = batch['input_tensor'].to(device)\n",
    "            target_tensor = batch['output_tensor'].transpose(1,0).to(device)\n",
    "            \n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder, decoder,\n",
    "                         encoder_optimizer, decoder_optimizer, criterion)\n",
    "            \n",
    "            loss_avg += loss\n",
    "            if i % print_every_n_batches == 0 and i != 0:\n",
    "                loss_avg /= print_every_n_batches\n",
    "                print(f'After {i} batches, average loss/{print_every_n_batches} batches: {loss_avg}')\n",
    "                plot_losses.append(loss)\n",
    "                loss_avg = 0\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters and construct dataloader\n",
    "hidden_size = 256\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(bbtDataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_gru = EncoderBiGRU(hidden_size, vectors).to(device)\n",
    "decoder_gru = AttnDecoderGRU(hidden_size, vectors, max_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model from scratch.\n"
     ]
    }
   ],
   "source": [
    "from_scratch = True # Set to False if you have saved weights and want to load them\n",
    "\n",
    "if not from_scratch:\n",
    "        # Load weights from earlier model\n",
    "    encoder_gru_state_dict = torch.load('encoder1_gru.pth')\n",
    "    decoder_gru_state_dict = torch.load('decoder1_gru.pth')\n",
    "\n",
    "    encoder_gru.load_state_dict(encoder_gru_state_dict)\n",
    "    decoder_gru.load_state_dict(decoder_gru_state_dict)\n",
    "else:\n",
    "    print('Training model from scratch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GRU based network.\n",
      "Epoch 1/47\n",
      "After 100 batches, average loss/100 batches: 109.56063678741455\n",
      "After 200 batches, average loss/100 batches: 105.89036998748779\n",
      "After 300 batches, average loss/100 batches: 106.4497661972046\n",
      "After 400 batches, average loss/100 batches: 106.74870559692383\n",
      "After 500 batches, average loss/100 batches: 106.91029273986817\n",
      "After 600 batches, average loss/100 batches: 111.22699851989746\n",
      "After 700 batches, average loss/100 batches: 110.87524394989013\n",
      "After 800 batches, average loss/100 batches: 106.74352241516114\n",
      "After 900 batches, average loss/100 batches: 107.70466983795166\n",
      "After 1000 batches, average loss/100 batches: 104.62533634185792\n",
      "After 1100 batches, average loss/100 batches: 103.82937576293945\n",
      "After 1200 batches, average loss/100 batches: 106.01955200195313\n",
      "After 1300 batches, average loss/100 batches: 108.12676761627198\n",
      "After 1400 batches, average loss/100 batches: 106.81457172393799\n",
      "After 1500 batches, average loss/100 batches: 107.22124481201172\n",
      "After 1600 batches, average loss/100 batches: 106.28377395629883\n",
      "After 1700 batches, average loss/100 batches: 106.31863372802735\n",
      "After 1800 batches, average loss/100 batches: 106.04998527526855\n",
      "After 1900 batches, average loss/100 batches: 107.0270026397705\n",
      "After 2000 batches, average loss/100 batches: 107.34467475891114\n",
      "After 2100 batches, average loss/100 batches: 104.02426830291748\n",
      "After 2200 batches, average loss/100 batches: 108.59092555999756\n",
      "After 2300 batches, average loss/100 batches: 105.43945514678956\n",
      "After 2400 batches, average loss/100 batches: 106.19384506225586\n",
      "Epoch 2/47\n",
      "After 100 batches, average loss/100 batches: 109.29957427978516\n",
      "After 200 batches, average loss/100 batches: 104.4031672668457\n",
      "After 300 batches, average loss/100 batches: 101.31551181793213\n",
      "After 400 batches, average loss/100 batches: 100.94744560241699\n",
      "After 500 batches, average loss/100 batches: 101.75535568237305\n",
      "After 600 batches, average loss/100 batches: 103.87527935028076\n",
      "After 700 batches, average loss/100 batches: 105.21141990661621\n",
      "After 800 batches, average loss/100 batches: 104.47848449707031\n",
      "After 900 batches, average loss/100 batches: 104.76298347473144\n",
      "After 1000 batches, average loss/100 batches: 105.47186637878418\n",
      "After 1100 batches, average loss/100 batches: 103.25625385284424\n",
      "After 1200 batches, average loss/100 batches: 105.07905017852784\n",
      "After 1300 batches, average loss/100 batches: 104.5779165649414\n",
      "After 1400 batches, average loss/100 batches: 108.24063652038575\n",
      "After 1500 batches, average loss/100 batches: 107.69277481079102\n",
      "After 1600 batches, average loss/100 batches: 102.19431774139404\n",
      "After 1700 batches, average loss/100 batches: 108.54235137939453\n",
      "After 1800 batches, average loss/100 batches: 104.6267535018921\n",
      "After 1900 batches, average loss/100 batches: 104.89442764282227\n",
      "After 2000 batches, average loss/100 batches: 106.83849697113037\n",
      "After 2100 batches, average loss/100 batches: 106.49522102355957\n",
      "After 2200 batches, average loss/100 batches: 103.20675060272217\n",
      "After 2300 batches, average loss/100 batches: 103.93053787231446\n",
      "After 2400 batches, average loss/100 batches: 104.06825431823731\n",
      "Epoch 3/47\n",
      "After 100 batches, average loss/100 batches: 104.13110477447509\n",
      "After 200 batches, average loss/100 batches: 102.73636241912841\n",
      "After 300 batches, average loss/100 batches: 101.9182014465332\n",
      "After 400 batches, average loss/100 batches: 103.99198246002197\n",
      "After 500 batches, average loss/100 batches: 100.60264293670654\n",
      "After 600 batches, average loss/100 batches: 104.0745304107666\n",
      "After 700 batches, average loss/100 batches: 106.31387184143067\n",
      "After 800 batches, average loss/100 batches: 105.94088245391846\n",
      "After 900 batches, average loss/100 batches: 101.83251560211181\n",
      "After 1000 batches, average loss/100 batches: 104.21767440795898\n",
      "After 1100 batches, average loss/100 batches: 106.10064468383788\n",
      "After 1200 batches, average loss/100 batches: 103.2860552597046\n",
      "After 1300 batches, average loss/100 batches: 103.47202655792236\n",
      "After 1400 batches, average loss/100 batches: 104.7903134918213\n",
      "After 1500 batches, average loss/100 batches: 101.98511646270752\n",
      "After 1600 batches, average loss/100 batches: 105.0078364944458\n",
      "After 1700 batches, average loss/100 batches: 107.45519821166992\n",
      "After 1800 batches, average loss/100 batches: 101.37779064178467\n",
      "After 1900 batches, average loss/100 batches: 107.07108947753906\n",
      "After 2000 batches, average loss/100 batches: 106.3746762084961\n",
      "After 2100 batches, average loss/100 batches: 104.44033908843994\n",
      "After 2200 batches, average loss/100 batches: 102.07799884796142\n",
      "After 2300 batches, average loss/100 batches: 102.1155874633789\n",
      "After 2400 batches, average loss/100 batches: 102.43154640197754\n",
      "Epoch 4/47\n",
      "After 100 batches, average loss/100 batches: 104.5638196182251\n",
      "After 200 batches, average loss/100 batches: 99.30967292785644\n",
      "After 300 batches, average loss/100 batches: 102.9386946105957\n",
      "After 400 batches, average loss/100 batches: 100.27682266235351\n",
      "After 500 batches, average loss/100 batches: 108.20129219055175\n",
      "After 600 batches, average loss/100 batches: 102.50545742034912\n",
      "After 700 batches, average loss/100 batches: 106.38815345764161\n",
      "After 800 batches, average loss/100 batches: 102.54902885437012\n",
      "After 900 batches, average loss/100 batches: 100.43707035064698\n",
      "After 1000 batches, average loss/100 batches: 101.37077869415283\n",
      "After 1100 batches, average loss/100 batches: 100.53510627746581\n",
      "After 1200 batches, average loss/100 batches: 103.48275814056396\n",
      "After 1300 batches, average loss/100 batches: 101.04826320648193\n",
      "After 1400 batches, average loss/100 batches: 98.91072658538819\n",
      "After 1500 batches, average loss/100 batches: 105.08402851104736\n",
      "After 1600 batches, average loss/100 batches: 103.71521644592285\n",
      "After 1700 batches, average loss/100 batches: 102.44050731658936\n",
      "After 1800 batches, average loss/100 batches: 101.93798469543457\n",
      "After 1900 batches, average loss/100 batches: 101.81300735473633\n",
      "After 2000 batches, average loss/100 batches: 101.95551803588867\n",
      "After 2100 batches, average loss/100 batches: 100.47895519256592\n",
      "After 2200 batches, average loss/100 batches: 103.70090869903565\n",
      "After 2300 batches, average loss/100 batches: 100.138415184021\n",
      "After 2400 batches, average loss/100 batches: 102.85630687713623\n",
      "Epoch 5/47\n",
      "After 100 batches, average loss/100 batches: 107.42477870941163\n",
      "After 200 batches, average loss/100 batches: 102.30152801513673\n",
      "After 300 batches, average loss/100 batches: 101.93321296691894\n",
      "After 400 batches, average loss/100 batches: 106.1029535293579\n",
      "After 500 batches, average loss/100 batches: 103.42515941619872\n",
      "After 600 batches, average loss/100 batches: 102.74083644866943\n",
      "After 700 batches, average loss/100 batches: 96.98891624450684\n",
      "After 800 batches, average loss/100 batches: 101.76649257659912\n",
      "After 900 batches, average loss/100 batches: 101.09514163970947\n",
      "After 1000 batches, average loss/100 batches: 103.1952261352539\n",
      "After 1100 batches, average loss/100 batches: 101.6964849472046\n",
      "After 1200 batches, average loss/100 batches: 101.818801612854\n",
      "After 1300 batches, average loss/100 batches: 101.25800636291504\n",
      "After 1400 batches, average loss/100 batches: 103.26860275268555\n",
      "After 1500 batches, average loss/100 batches: 102.54436973571778\n",
      "After 1600 batches, average loss/100 batches: 103.88389675140381\n",
      "After 1700 batches, average loss/100 batches: 99.64536022186279\n",
      "After 1800 batches, average loss/100 batches: 102.97930576324462\n",
      "After 1900 batches, average loss/100 batches: 101.83042575836181\n",
      "After 2000 batches, average loss/100 batches: 102.69065307617187\n",
      "After 2100 batches, average loss/100 batches: 102.1460514831543\n",
      "After 2200 batches, average loss/100 batches: 103.22731742858886\n",
      "After 2300 batches, average loss/100 batches: 107.14740734100342\n",
      "After 2400 batches, average loss/100 batches: 103.81561695098877\n",
      "Epoch 6/47\n",
      "After 100 batches, average loss/100 batches: 98.0391658782959\n",
      "After 200 batches, average loss/100 batches: 99.05338794708251\n",
      "After 300 batches, average loss/100 batches: 102.14937850952148\n",
      "After 400 batches, average loss/100 batches: 103.58283477783203\n",
      "After 500 batches, average loss/100 batches: 102.83638465881347\n",
      "After 600 batches, average loss/100 batches: 100.74951042175293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 700 batches, average loss/100 batches: 104.47098751068116\n",
      "After 800 batches, average loss/100 batches: 98.26775093078614\n",
      "After 900 batches, average loss/100 batches: 102.76000801086425\n",
      "After 1000 batches, average loss/100 batches: 104.90059677124023\n",
      "After 1100 batches, average loss/100 batches: 102.4737738418579\n",
      "After 1200 batches, average loss/100 batches: 102.22017658233642\n",
      "After 1300 batches, average loss/100 batches: 105.48122699737549\n",
      "After 1400 batches, average loss/100 batches: 103.04278522491455\n",
      "After 1500 batches, average loss/100 batches: 101.58248489379883\n",
      "After 1600 batches, average loss/100 batches: 102.53911865234375\n",
      "After 1700 batches, average loss/100 batches: 104.29577617645263\n",
      "After 1800 batches, average loss/100 batches: 104.21519512176513\n",
      "After 1900 batches, average loss/100 batches: 98.20664264678955\n",
      "After 2000 batches, average loss/100 batches: 104.42839248657226\n",
      "After 2100 batches, average loss/100 batches: 103.40879444122315\n",
      "After 2200 batches, average loss/100 batches: 104.66725360870362\n",
      "After 2300 batches, average loss/100 batches: 97.78302555084228\n",
      "After 2400 batches, average loss/100 batches: 104.42492012023926\n",
      "Epoch 7/47\n",
      "After 100 batches, average loss/100 batches: 99.77692211151123\n",
      "After 200 batches, average loss/100 batches: 103.55089557647705\n",
      "After 300 batches, average loss/100 batches: 97.29722312927247\n",
      "After 400 batches, average loss/100 batches: 101.29723579406738\n",
      "After 500 batches, average loss/100 batches: 100.59926872253418\n",
      "After 600 batches, average loss/100 batches: 101.42848747253419\n",
      "After 700 batches, average loss/100 batches: 99.71252490997314\n",
      "After 800 batches, average loss/100 batches: 103.2110767364502\n",
      "After 900 batches, average loss/100 batches: 105.312236328125\n",
      "After 1000 batches, average loss/100 batches: 98.42567100524903\n",
      "After 1100 batches, average loss/100 batches: 100.50865791320801\n",
      "After 1200 batches, average loss/100 batches: 99.48336402893067\n",
      "After 1300 batches, average loss/100 batches: 97.50451160430909\n",
      "After 1400 batches, average loss/100 batches: 100.79888710021973\n",
      "After 1500 batches, average loss/100 batches: 100.14279453277588\n",
      "After 1600 batches, average loss/100 batches: 99.58854961395264\n",
      "After 1700 batches, average loss/100 batches: 100.05285648345948\n",
      "After 1800 batches, average loss/100 batches: 101.87892330169677\n",
      "After 1900 batches, average loss/100 batches: 97.70926494598389\n",
      "After 2000 batches, average loss/100 batches: 100.9524264907837\n",
      "After 2100 batches, average loss/100 batches: 101.56873809814454\n",
      "After 2200 batches, average loss/100 batches: 100.93364910125733\n",
      "After 2300 batches, average loss/100 batches: 104.09168041229248\n",
      "After 2400 batches, average loss/100 batches: 101.45397106170654\n",
      "Epoch 8/47\n",
      "After 100 batches, average loss/100 batches: 104.08966011047363\n",
      "After 200 batches, average loss/100 batches: 100.15337017059326\n",
      "After 300 batches, average loss/100 batches: 94.83172969818115\n",
      "After 400 batches, average loss/100 batches: 98.92846965789795\n",
      "After 500 batches, average loss/100 batches: 103.31640232086181\n",
      "After 600 batches, average loss/100 batches: 104.5223025894165\n",
      "After 700 batches, average loss/100 batches: 99.26928161621093\n",
      "After 800 batches, average loss/100 batches: 99.89616970062256\n",
      "After 900 batches, average loss/100 batches: 98.33710849761962\n",
      "After 1000 batches, average loss/100 batches: 103.15361206054688\n",
      "After 1100 batches, average loss/100 batches: 100.90461433410644\n",
      "After 1200 batches, average loss/100 batches: 104.01920177459716\n",
      "After 1300 batches, average loss/100 batches: 98.61250061035156\n",
      "After 1400 batches, average loss/100 batches: 101.03321933746338\n",
      "After 1500 batches, average loss/100 batches: 103.33573741912842\n",
      "After 1600 batches, average loss/100 batches: 103.46764442443848\n",
      "After 1700 batches, average loss/100 batches: 103.30066627502441\n",
      "After 1800 batches, average loss/100 batches: 100.64581882476807\n",
      "After 1900 batches, average loss/100 batches: 104.6165668106079\n",
      "After 2000 batches, average loss/100 batches: 98.84696060180664\n",
      "After 2100 batches, average loss/100 batches: 98.01448890686035\n",
      "After 2200 batches, average loss/100 batches: 93.69563346862793\n",
      "After 2300 batches, average loss/100 batches: 96.79860012054444\n",
      "After 2400 batches, average loss/100 batches: 100.1075862121582\n",
      "Epoch 9/47\n",
      "After 100 batches, average loss/100 batches: 102.26148078918457\n",
      "After 200 batches, average loss/100 batches: 94.56771755218506\n",
      "After 300 batches, average loss/100 batches: 100.00442234039306\n",
      "After 400 batches, average loss/100 batches: 100.4796152496338\n",
      "After 500 batches, average loss/100 batches: 95.40736198425293\n",
      "After 600 batches, average loss/100 batches: 104.47270683288575\n",
      "After 700 batches, average loss/100 batches: 96.91901908874512\n",
      "After 800 batches, average loss/100 batches: 103.10201362609864\n",
      "After 900 batches, average loss/100 batches: 100.56977840423583\n",
      "After 1000 batches, average loss/100 batches: 102.44003368377686\n",
      "After 1100 batches, average loss/100 batches: 99.08114212036133\n",
      "After 1200 batches, average loss/100 batches: 93.67109909057618\n",
      "After 1300 batches, average loss/100 batches: 100.6071033859253\n",
      "After 1400 batches, average loss/100 batches: 100.42755157470702\n",
      "After 1500 batches, average loss/100 batches: 97.02801483154298\n",
      "After 1600 batches, average loss/100 batches: 101.07515602111816\n",
      "After 1700 batches, average loss/100 batches: 100.53567497253418\n",
      "After 1800 batches, average loss/100 batches: 104.29970512390136\n",
      "After 1900 batches, average loss/100 batches: 99.92883155822754\n",
      "After 2000 batches, average loss/100 batches: 97.21795967102051\n",
      "After 2100 batches, average loss/100 batches: 100.07193374633789\n",
      "After 2200 batches, average loss/100 batches: 99.35603126525879\n",
      "After 2300 batches, average loss/100 batches: 97.64373523712158\n",
      "After 2400 batches, average loss/100 batches: 102.81186393737794\n",
      "Epoch 10/47\n",
      "After 100 batches, average loss/100 batches: 99.58793640136719\n",
      "After 200 batches, average loss/100 batches: 100.97838516235352\n",
      "After 300 batches, average loss/100 batches: 101.04957180023193\n",
      "After 400 batches, average loss/100 batches: 98.39287059783936\n",
      "After 500 batches, average loss/100 batches: 100.5800938796997\n",
      "After 600 batches, average loss/100 batches: 96.39388076782227\n",
      "After 700 batches, average loss/100 batches: 99.49712333679199\n",
      "After 800 batches, average loss/100 batches: 95.57885375976562\n",
      "After 900 batches, average loss/100 batches: 98.236664352417\n",
      "After 1000 batches, average loss/100 batches: 99.75631637573242\n",
      "After 1100 batches, average loss/100 batches: 96.89095836639405\n",
      "After 1200 batches, average loss/100 batches: 102.07221157073974\n",
      "After 1300 batches, average loss/100 batches: 102.71126388549804\n",
      "After 1400 batches, average loss/100 batches: 102.16628669738769\n",
      "After 1500 batches, average loss/100 batches: 96.23747093200683\n",
      "After 1600 batches, average loss/100 batches: 99.61386672973633\n",
      "After 1700 batches, average loss/100 batches: 100.69556102752685\n",
      "After 1800 batches, average loss/100 batches: 97.19010627746582\n",
      "After 1900 batches, average loss/100 batches: 100.85983596801758\n",
      "After 2000 batches, average loss/100 batches: 97.5312899017334\n",
      "After 2100 batches, average loss/100 batches: 96.52947196960449\n",
      "After 2200 batches, average loss/100 batches: 98.69196071624756\n",
      "After 2300 batches, average loss/100 batches: 100.78974304199218\n",
      "After 2400 batches, average loss/100 batches: 102.21634662628173\n",
      "Epoch 11/47\n",
      "After 100 batches, average loss/100 batches: 100.40899074554443\n",
      "After 200 batches, average loss/100 batches: 96.31429622650147\n",
      "After 300 batches, average loss/100 batches: 103.16904750823974\n",
      "After 400 batches, average loss/100 batches: 99.31825702667237\n",
      "After 500 batches, average loss/100 batches: 98.92951065063477\n",
      "After 600 batches, average loss/100 batches: 95.76496013641358\n",
      "After 700 batches, average loss/100 batches: 98.31746528625489\n",
      "After 800 batches, average loss/100 batches: 100.53233757019044\n",
      "After 900 batches, average loss/100 batches: 99.77965660095215\n",
      "After 1000 batches, average loss/100 batches: 94.43813327789307\n",
      "After 1100 batches, average loss/100 batches: 96.3111358642578\n",
      "After 1200 batches, average loss/100 batches: 100.9085852432251\n",
      "After 1300 batches, average loss/100 batches: 98.61399677276611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1400 batches, average loss/100 batches: 100.72310012817383\n",
      "After 1500 batches, average loss/100 batches: 98.64979553222656\n",
      "After 1600 batches, average loss/100 batches: 99.08288402557373\n",
      "After 1700 batches, average loss/100 batches: 97.28545463562011\n",
      "After 1800 batches, average loss/100 batches: 102.77111045837403\n",
      "After 1900 batches, average loss/100 batches: 97.23604354858398\n",
      "After 2000 batches, average loss/100 batches: 94.54350360870362\n",
      "After 2100 batches, average loss/100 batches: 103.3486206817627\n",
      "After 2200 batches, average loss/100 batches: 98.23368495941162\n",
      "After 2300 batches, average loss/100 batches: 99.9844435119629\n",
      "After 2400 batches, average loss/100 batches: 99.92993698120117\n",
      "Epoch 12/47\n",
      "After 100 batches, average loss/100 batches: 101.61166446685792\n",
      "After 200 batches, average loss/100 batches: 98.45233192443848\n",
      "After 300 batches, average loss/100 batches: 99.06790744781495\n",
      "After 400 batches, average loss/100 batches: 97.74601657867431\n",
      "After 500 batches, average loss/100 batches: 94.81909561157227\n",
      "After 600 batches, average loss/100 batches: 100.70075485229492\n",
      "After 700 batches, average loss/100 batches: 103.98263938903808\n",
      "After 800 batches, average loss/100 batches: 101.73923484802246\n",
      "After 900 batches, average loss/100 batches: 97.79731647491455\n",
      "After 1000 batches, average loss/100 batches: 96.1821097946167\n",
      "After 1100 batches, average loss/100 batches: 96.19645015716553\n",
      "After 1200 batches, average loss/100 batches: 96.54685680389404\n",
      "After 1300 batches, average loss/100 batches: 100.45733009338379\n",
      "After 1400 batches, average loss/100 batches: 98.0944923400879\n",
      "After 1500 batches, average loss/100 batches: 95.89873027801514\n",
      "After 1600 batches, average loss/100 batches: 96.56479221343994\n",
      "After 1700 batches, average loss/100 batches: 96.31738536834717\n",
      "After 1800 batches, average loss/100 batches: 99.21957248687744\n",
      "After 1900 batches, average loss/100 batches: 95.0814895248413\n",
      "After 2000 batches, average loss/100 batches: 100.73892951965333\n",
      "After 2100 batches, average loss/100 batches: 97.03118602752686\n",
      "After 2200 batches, average loss/100 batches: 101.6268970489502\n",
      "After 2300 batches, average loss/100 batches: 99.02266132354737\n",
      "After 2400 batches, average loss/100 batches: 98.0439826965332\n",
      "Epoch 13/47\n",
      "After 100 batches, average loss/100 batches: 102.63866889953613\n",
      "After 200 batches, average loss/100 batches: 97.72619438171387\n",
      "After 300 batches, average loss/100 batches: 95.8117339706421\n",
      "After 400 batches, average loss/100 batches: 94.50482986450196\n",
      "After 500 batches, average loss/100 batches: 100.93575649261474\n",
      "After 600 batches, average loss/100 batches: 93.86056823730469\n",
      "After 700 batches, average loss/100 batches: 97.95719570159912\n",
      "After 800 batches, average loss/100 batches: 94.64060375213623\n",
      "After 900 batches, average loss/100 batches: 104.39313961029053\n",
      "After 1000 batches, average loss/100 batches: 97.25496044158936\n",
      "After 1100 batches, average loss/100 batches: 100.71495235443115\n",
      "After 1200 batches, average loss/100 batches: 99.4973722076416\n",
      "After 1300 batches, average loss/100 batches: 96.79014881134033\n",
      "After 1400 batches, average loss/100 batches: 99.04564781188965\n",
      "After 1500 batches, average loss/100 batches: 102.85516132354736\n",
      "After 1600 batches, average loss/100 batches: 97.53644008636475\n",
      "After 1700 batches, average loss/100 batches: 101.96289070129394\n",
      "After 1800 batches, average loss/100 batches: 96.41943099975586\n",
      "After 1900 batches, average loss/100 batches: 98.31982601165771\n",
      "After 2000 batches, average loss/100 batches: 100.50117591857911\n",
      "After 2100 batches, average loss/100 batches: 99.98770782470703\n",
      "After 2200 batches, average loss/100 batches: 97.55590816497802\n",
      "After 2300 batches, average loss/100 batches: 98.87909969329834\n",
      "After 2400 batches, average loss/100 batches: 95.11959987640381\n",
      "Epoch 14/47\n",
      "After 100 batches, average loss/100 batches: 94.52740066528321\n",
      "After 200 batches, average loss/100 batches: 98.7855784225464\n",
      "After 300 batches, average loss/100 batches: 98.93950004577637\n",
      "After 400 batches, average loss/100 batches: 100.43522911071777\n",
      "After 500 batches, average loss/100 batches: 99.04703834533692\n",
      "After 600 batches, average loss/100 batches: 98.11630229949951\n",
      "After 700 batches, average loss/100 batches: 99.57574893951416\n",
      "After 800 batches, average loss/100 batches: 99.08675533294678\n",
      "After 900 batches, average loss/100 batches: 95.44461307525634\n",
      "After 1000 batches, average loss/100 batches: 95.9509262084961\n",
      "After 1100 batches, average loss/100 batches: 95.3433226776123\n",
      "After 1200 batches, average loss/100 batches: 98.08468521118164\n",
      "After 1300 batches, average loss/100 batches: 96.63282730102539\n",
      "After 1400 batches, average loss/100 batches: 96.13462810516357\n",
      "After 1500 batches, average loss/100 batches: 94.73970340728759\n",
      "After 1600 batches, average loss/100 batches: 102.98719963073731\n",
      "After 1700 batches, average loss/100 batches: 95.2618278503418\n",
      "After 1800 batches, average loss/100 batches: 96.82647640228271\n",
      "After 1900 batches, average loss/100 batches: 96.53436485290527\n",
      "After 2000 batches, average loss/100 batches: 97.20494897842407\n",
      "After 2100 batches, average loss/100 batches: 102.66579067230225\n",
      "After 2200 batches, average loss/100 batches: 97.0317978668213\n",
      "After 2300 batches, average loss/100 batches: 97.3751777267456\n",
      "After 2400 batches, average loss/100 batches: 100.30833305358887\n",
      "Epoch 15/47\n",
      "After 100 batches, average loss/100 batches: 100.6330729675293\n",
      "After 200 batches, average loss/100 batches: 95.11174331665039\n",
      "After 300 batches, average loss/100 batches: 99.54123481750489\n",
      "After 400 batches, average loss/100 batches: 95.77836471557617\n",
      "After 500 batches, average loss/100 batches: 95.04826656341552\n",
      "After 600 batches, average loss/100 batches: 95.52027912139893\n",
      "After 700 batches, average loss/100 batches: 101.37778747558593\n",
      "After 800 batches, average loss/100 batches: 97.4213053894043\n",
      "After 900 batches, average loss/100 batches: 99.80010433197022\n",
      "After 1000 batches, average loss/100 batches: 97.60727111816406\n",
      "After 1100 batches, average loss/100 batches: 98.95592044830322\n",
      "After 1200 batches, average loss/100 batches: 93.62901741027832\n",
      "After 1300 batches, average loss/100 batches: 97.96789707183838\n",
      "After 1400 batches, average loss/100 batches: 98.67469356536866\n",
      "After 1500 batches, average loss/100 batches: 100.58454555511474\n",
      "After 1600 batches, average loss/100 batches: 99.10516994476319\n",
      "After 1700 batches, average loss/100 batches: 100.26218307495117\n",
      "After 1800 batches, average loss/100 batches: 97.84049922943115\n",
      "After 1900 batches, average loss/100 batches: 96.80384185791016\n",
      "After 2000 batches, average loss/100 batches: 98.48704826354981\n",
      "After 2100 batches, average loss/100 batches: 97.26019512176514\n",
      "After 2200 batches, average loss/100 batches: 101.68217712402344\n",
      "After 2300 batches, average loss/100 batches: 97.48743156433106\n",
      "After 2400 batches, average loss/100 batches: 95.58228496551514\n",
      "Epoch 16/47\n",
      "After 100 batches, average loss/100 batches: 94.66131443023681\n",
      "After 200 batches, average loss/100 batches: 93.48362579345704\n",
      "After 300 batches, average loss/100 batches: 99.19603675842285\n",
      "After 400 batches, average loss/100 batches: 95.9033736038208\n",
      "After 500 batches, average loss/100 batches: 102.02581214904785\n",
      "After 600 batches, average loss/100 batches: 94.83451194763184\n",
      "After 700 batches, average loss/100 batches: 98.09404521942139\n",
      "After 800 batches, average loss/100 batches: 97.89278530120849\n",
      "After 900 batches, average loss/100 batches: 94.67333740234375\n",
      "After 1000 batches, average loss/100 batches: 102.50932285308838\n",
      "After 1100 batches, average loss/100 batches: 98.45893817901612\n",
      "After 1200 batches, average loss/100 batches: 98.70356803894043\n",
      "After 1300 batches, average loss/100 batches: 94.01923347473145\n",
      "After 1400 batches, average loss/100 batches: 97.98589393615723\n",
      "After 1500 batches, average loss/100 batches: 95.04666145324707\n",
      "After 1600 batches, average loss/100 batches: 98.90761966705323\n",
      "After 1700 batches, average loss/100 batches: 94.22318214416504\n",
      "After 1800 batches, average loss/100 batches: 101.84549041748046\n",
      "After 1900 batches, average loss/100 batches: 94.94931198120118\n",
      "After 2000 batches, average loss/100 batches: 93.99401187896729\n",
      "After 2100 batches, average loss/100 batches: 98.98505756378174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2200 batches, average loss/100 batches: 98.8921768951416\n",
      "After 2300 batches, average loss/100 batches: 96.3934049987793\n",
      "After 2400 batches, average loss/100 batches: 98.20318386077881\n",
      "Epoch 17/47\n",
      "After 100 batches, average loss/100 batches: 99.17813907623291\n",
      "After 200 batches, average loss/100 batches: 96.33238723754883\n",
      "After 300 batches, average loss/100 batches: 93.6120894241333\n",
      "After 400 batches, average loss/100 batches: 96.07028602600097\n",
      "After 500 batches, average loss/100 batches: 99.86515686035156\n",
      "After 600 batches, average loss/100 batches: 97.29045688629151\n",
      "After 700 batches, average loss/100 batches: 96.82001003265381\n",
      "After 800 batches, average loss/100 batches: 97.43315044403076\n",
      "After 900 batches, average loss/100 batches: 103.61631141662598\n",
      "After 1000 batches, average loss/100 batches: 97.01843246459961\n",
      "After 1100 batches, average loss/100 batches: 96.03460060119629\n",
      "After 1200 batches, average loss/100 batches: 93.68364624023438\n",
      "After 1300 batches, average loss/100 batches: 98.58812881469727\n",
      "After 1400 batches, average loss/100 batches: 95.65076084136963\n",
      "After 1500 batches, average loss/100 batches: 97.18791877746582\n",
      "After 1600 batches, average loss/100 batches: 95.14326221466064\n",
      "After 1700 batches, average loss/100 batches: 98.34942905426026\n",
      "After 1800 batches, average loss/100 batches: 94.59829723358155\n",
      "After 1900 batches, average loss/100 batches: 98.5149931716919\n",
      "After 2000 batches, average loss/100 batches: 96.32666259765625\n",
      "After 2100 batches, average loss/100 batches: 96.85970581054687\n",
      "After 2200 batches, average loss/100 batches: 96.81027107238769\n",
      "After 2300 batches, average loss/100 batches: 96.01459133148194\n",
      "After 2400 batches, average loss/100 batches: 92.39995597839355\n",
      "Epoch 18/47\n",
      "After 100 batches, average loss/100 batches: 98.57324558258057\n",
      "After 200 batches, average loss/100 batches: 91.88876098632812\n",
      "After 300 batches, average loss/100 batches: 98.111526222229\n",
      "After 400 batches, average loss/100 batches: 91.26972553253174\n",
      "After 500 batches, average loss/100 batches: 97.27896659851075\n",
      "After 600 batches, average loss/100 batches: 98.90836605072022\n",
      "After 700 batches, average loss/100 batches: 97.0877233505249\n",
      "After 800 batches, average loss/100 batches: 93.01235427856446\n",
      "After 900 batches, average loss/100 batches: 93.93542621612549\n",
      "After 1000 batches, average loss/100 batches: 97.52563682556152\n",
      "After 1100 batches, average loss/100 batches: 97.4684666442871\n",
      "After 1200 batches, average loss/100 batches: 95.50144409179687\n",
      "After 1300 batches, average loss/100 batches: 96.07115032196045\n",
      "After 1400 batches, average loss/100 batches: 96.37353736877441\n",
      "After 1500 batches, average loss/100 batches: 100.02557270050049\n",
      "After 1600 batches, average loss/100 batches: 98.44868057250977\n",
      "After 1700 batches, average loss/100 batches: 97.99536304473877\n",
      "After 1800 batches, average loss/100 batches: 97.62993675231934\n",
      "After 1900 batches, average loss/100 batches: 97.99096607208251\n",
      "After 2000 batches, average loss/100 batches: 95.31634113311767\n",
      "After 2100 batches, average loss/100 batches: 95.51054656982421\n",
      "After 2200 batches, average loss/100 batches: 96.4658141708374\n",
      "After 2300 batches, average loss/100 batches: 95.347170753479\n",
      "After 2400 batches, average loss/100 batches: 94.16863510131836\n",
      "Epoch 19/47\n",
      "After 100 batches, average loss/100 batches: 95.52716335296631\n",
      "After 200 batches, average loss/100 batches: 98.17854984283447\n",
      "After 300 batches, average loss/100 batches: 93.14520629882813\n",
      "After 400 batches, average loss/100 batches: 95.46011695861816\n",
      "After 500 batches, average loss/100 batches: 91.72601196289062\n",
      "After 600 batches, average loss/100 batches: 98.5403634262085\n",
      "After 700 batches, average loss/100 batches: 102.57448268890381\n",
      "After 800 batches, average loss/100 batches: 97.62912532806396\n",
      "After 900 batches, average loss/100 batches: 94.10321235656738\n",
      "After 1000 batches, average loss/100 batches: 95.9336498260498\n",
      "After 1100 batches, average loss/100 batches: 97.23540229797364\n",
      "After 1200 batches, average loss/100 batches: 96.78143138885498\n",
      "After 1300 batches, average loss/100 batches: 98.73397022247315\n",
      "After 1400 batches, average loss/100 batches: 90.30207691192626\n",
      "After 1500 batches, average loss/100 batches: 96.01500560760498\n",
      "After 1600 batches, average loss/100 batches: 93.7884764099121\n",
      "After 1700 batches, average loss/100 batches: 93.84550449371338\n",
      "After 1800 batches, average loss/100 batches: 96.58631034851074\n",
      "After 1900 batches, average loss/100 batches: 96.87946556091309\n",
      "After 2000 batches, average loss/100 batches: 97.83571464538574\n",
      "After 2100 batches, average loss/100 batches: 98.081481590271\n",
      "After 2200 batches, average loss/100 batches: 96.42631286621094\n",
      "After 2300 batches, average loss/100 batches: 94.6667206954956\n",
      "After 2400 batches, average loss/100 batches: 95.72394710540772\n",
      "Epoch 20/47\n",
      "After 100 batches, average loss/100 batches: 94.16877220153809\n",
      "After 200 batches, average loss/100 batches: 96.95751724243163\n",
      "After 300 batches, average loss/100 batches: 89.65994457244874\n",
      "After 400 batches, average loss/100 batches: 92.44758808135987\n",
      "After 500 batches, average loss/100 batches: 95.69359069824219\n",
      "After 600 batches, average loss/100 batches: 93.13095447540283\n",
      "After 700 batches, average loss/100 batches: 94.51206367492676\n",
      "After 800 batches, average loss/100 batches: 93.31153148651123\n",
      "After 900 batches, average loss/100 batches: 94.11289581298828\n",
      "After 1000 batches, average loss/100 batches: 96.68689109802246\n",
      "After 1100 batches, average loss/100 batches: 97.32336891174316\n",
      "After 1200 batches, average loss/100 batches: 94.29182117462159\n",
      "After 1300 batches, average loss/100 batches: 97.79156509399414\n",
      "After 1400 batches, average loss/100 batches: 98.22761756896972\n",
      "After 1500 batches, average loss/100 batches: 94.17959045410156\n",
      "After 1600 batches, average loss/100 batches: 97.81989181518554\n",
      "After 1700 batches, average loss/100 batches: 98.58648696899414\n",
      "After 1800 batches, average loss/100 batches: 97.2746167755127\n",
      "After 1900 batches, average loss/100 batches: 94.87020687103272\n",
      "After 2000 batches, average loss/100 batches: 98.04083812713623\n",
      "After 2100 batches, average loss/100 batches: 98.3465654373169\n",
      "After 2200 batches, average loss/100 batches: 95.37503105163574\n",
      "After 2300 batches, average loss/100 batches: 96.14839897155761\n",
      "After 2400 batches, average loss/100 batches: 98.09082637786865\n",
      "Epoch 21/47\n",
      "After 100 batches, average loss/100 batches: 96.53679294586182\n",
      "After 200 batches, average loss/100 batches: 90.88557235717774\n",
      "After 300 batches, average loss/100 batches: 96.11476047515869\n",
      "After 400 batches, average loss/100 batches: 95.89111602783203\n",
      "After 500 batches, average loss/100 batches: 96.10565505981445\n",
      "After 600 batches, average loss/100 batches: 91.57757038116455\n",
      "After 700 batches, average loss/100 batches: 99.19194484710694\n",
      "After 800 batches, average loss/100 batches: 95.28935279846192\n",
      "After 900 batches, average loss/100 batches: 97.08391338348389\n",
      "After 1000 batches, average loss/100 batches: 91.71432174682617\n",
      "After 1100 batches, average loss/100 batches: 94.70954849243164\n",
      "After 1200 batches, average loss/100 batches: 94.54358871459961\n",
      "After 1300 batches, average loss/100 batches: 97.1000520324707\n",
      "After 1400 batches, average loss/100 batches: 94.34546001434326\n",
      "After 1500 batches, average loss/100 batches: 94.17176574707031\n",
      "After 1600 batches, average loss/100 batches: 94.58740673065185\n",
      "After 1700 batches, average loss/100 batches: 91.85896221160888\n",
      "After 1800 batches, average loss/100 batches: 94.60070629119873\n",
      "After 1900 batches, average loss/100 batches: 90.4328252029419\n",
      "After 2000 batches, average loss/100 batches: 91.00393684387207\n",
      "After 2100 batches, average loss/100 batches: 96.02380558013915\n",
      "After 2200 batches, average loss/100 batches: 98.95024181365967\n",
      "After 2300 batches, average loss/100 batches: 91.28214855194092\n",
      "After 2400 batches, average loss/100 batches: 97.22114917755127\n",
      "Epoch 22/47\n",
      "After 100 batches, average loss/100 batches: 95.16673675537109\n",
      "After 200 batches, average loss/100 batches: 95.20577713012695\n",
      "After 300 batches, average loss/100 batches: 90.9196410369873\n",
      "After 400 batches, average loss/100 batches: 94.3219271850586\n",
      "After 500 batches, average loss/100 batches: 97.68750747680664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 600 batches, average loss/100 batches: 93.27973114013672\n",
      "After 700 batches, average loss/100 batches: 94.49757961273194\n",
      "After 800 batches, average loss/100 batches: 91.58651371002198\n",
      "After 900 batches, average loss/100 batches: 92.40607669830322\n",
      "After 1000 batches, average loss/100 batches: 97.27389980316163\n",
      "After 1100 batches, average loss/100 batches: 89.93472522735595\n",
      "After 1200 batches, average loss/100 batches: 93.47131542205811\n",
      "After 1300 batches, average loss/100 batches: 99.56501945495606\n",
      "After 1400 batches, average loss/100 batches: 94.99928058624268\n",
      "After 1500 batches, average loss/100 batches: 93.36105175018311\n",
      "After 1600 batches, average loss/100 batches: 99.74368019104004\n",
      "After 1700 batches, average loss/100 batches: 92.09591815948487\n",
      "After 1800 batches, average loss/100 batches: 91.16223461151122\n",
      "After 1900 batches, average loss/100 batches: 96.17536037445069\n",
      "After 2000 batches, average loss/100 batches: 97.55055141448975\n",
      "After 2100 batches, average loss/100 batches: 95.2654086303711\n",
      "After 2200 batches, average loss/100 batches: 95.05938598632812\n",
      "After 2300 batches, average loss/100 batches: 98.53534606933594\n",
      "After 2400 batches, average loss/100 batches: 100.02525638580322\n",
      "Epoch 23/47\n",
      "After 100 batches, average loss/100 batches: 92.71044452667236\n",
      "After 200 batches, average loss/100 batches: 91.08776443481446\n",
      "After 300 batches, average loss/100 batches: 95.20311477661133\n",
      "After 400 batches, average loss/100 batches: 95.26901748657227\n",
      "After 500 batches, average loss/100 batches: 89.59518703460694\n",
      "After 600 batches, average loss/100 batches: 91.07419017791749\n",
      "After 700 batches, average loss/100 batches: 92.79508625030518\n",
      "After 800 batches, average loss/100 batches: 90.52444110870361\n",
      "After 900 batches, average loss/100 batches: 98.01727905273438\n",
      "After 1000 batches, average loss/100 batches: 93.40812122344971\n",
      "After 1100 batches, average loss/100 batches: 93.18269336700439\n",
      "After 1200 batches, average loss/100 batches: 92.59980819702149\n",
      "After 1300 batches, average loss/100 batches: 94.31797817230225\n",
      "After 1400 batches, average loss/100 batches: 93.18821197509766\n",
      "After 1500 batches, average loss/100 batches: 93.83133407592773\n",
      "After 1600 batches, average loss/100 batches: 93.71651027679444\n",
      "After 1700 batches, average loss/100 batches: 99.50950763702393\n",
      "After 1800 batches, average loss/100 batches: 93.99225036621094\n",
      "After 1900 batches, average loss/100 batches: 95.50946273803712\n",
      "After 2000 batches, average loss/100 batches: 94.72722511291504\n",
      "After 2100 batches, average loss/100 batches: 94.81081188201904\n",
      "After 2200 batches, average loss/100 batches: 93.0851587677002\n",
      "After 2300 batches, average loss/100 batches: 94.03094673156738\n",
      "After 2400 batches, average loss/100 batches: 92.6850286102295\n",
      "Epoch 24/47\n",
      "After 100 batches, average loss/100 batches: 94.82818916320801\n",
      "After 200 batches, average loss/100 batches: 95.83014778137208\n",
      "After 300 batches, average loss/100 batches: 93.15199771881103\n",
      "After 400 batches, average loss/100 batches: 98.8234796142578\n",
      "After 500 batches, average loss/100 batches: 93.63662662506104\n",
      "After 600 batches, average loss/100 batches: 92.23169181823731\n",
      "After 700 batches, average loss/100 batches: 91.82686340332032\n",
      "After 800 batches, average loss/100 batches: 96.66242626190186\n",
      "After 900 batches, average loss/100 batches: 92.60489498138428\n",
      "After 1000 batches, average loss/100 batches: 93.79038475036621\n",
      "After 1100 batches, average loss/100 batches: 90.21470134735108\n",
      "After 1200 batches, average loss/100 batches: 97.68472183227539\n",
      "After 1300 batches, average loss/100 batches: 96.60366783142089\n",
      "After 1400 batches, average loss/100 batches: 95.23604167938232\n",
      "After 1500 batches, average loss/100 batches: 96.02713062286377\n",
      "After 1600 batches, average loss/100 batches: 91.22037956237793\n",
      "After 1700 batches, average loss/100 batches: 98.29224071502685\n",
      "After 1800 batches, average loss/100 batches: 91.14308200836182\n",
      "After 1900 batches, average loss/100 batches: 97.00314884185791\n",
      "After 2000 batches, average loss/100 batches: 90.93646270751952\n",
      "After 2100 batches, average loss/100 batches: 95.42282260894775\n",
      "After 2200 batches, average loss/100 batches: 92.73746826171875\n",
      "After 2300 batches, average loss/100 batches: 93.2298779296875\n",
      "After 2400 batches, average loss/100 batches: 93.32646911621094\n",
      "Epoch 25/47\n",
      "After 100 batches, average loss/100 batches: 96.04064296722412\n",
      "After 200 batches, average loss/100 batches: 93.43873565673829\n",
      "After 300 batches, average loss/100 batches: 90.03573055267334\n",
      "After 400 batches, average loss/100 batches: 89.38294246673584\n",
      "After 500 batches, average loss/100 batches: 93.05010597229004\n",
      "After 600 batches, average loss/100 batches: 91.43759712219239\n",
      "After 700 batches, average loss/100 batches: 91.45442321777344\n",
      "After 800 batches, average loss/100 batches: 92.06690391540528\n",
      "After 900 batches, average loss/100 batches: 92.73279506683349\n",
      "After 1000 batches, average loss/100 batches: 90.26371849060058\n",
      "After 1100 batches, average loss/100 batches: 94.24525917053222\n",
      "After 1200 batches, average loss/100 batches: 91.82734664916993\n",
      "After 1300 batches, average loss/100 batches: 95.0745011138916\n",
      "After 1400 batches, average loss/100 batches: 95.95056381225587\n",
      "After 1500 batches, average loss/100 batches: 90.47617290496827\n",
      "After 1600 batches, average loss/100 batches: 94.79044204711914\n",
      "After 1700 batches, average loss/100 batches: 95.72031467437745\n",
      "After 1800 batches, average loss/100 batches: 92.01563320159912\n",
      "After 1900 batches, average loss/100 batches: 94.73311710357666\n",
      "After 2000 batches, average loss/100 batches: 93.64283569335937\n",
      "After 2100 batches, average loss/100 batches: 91.55023216247558\n",
      "After 2200 batches, average loss/100 batches: 91.57813779830933\n",
      "After 2300 batches, average loss/100 batches: 91.00297958374023\n",
      "After 2400 batches, average loss/100 batches: 91.78515361785888\n",
      "Epoch 26/47\n",
      "After 100 batches, average loss/100 batches: 95.80788333892822\n",
      "After 200 batches, average loss/100 batches: 93.61622737884521\n",
      "After 300 batches, average loss/100 batches: 93.41500289916992\n",
      "After 400 batches, average loss/100 batches: 90.83093936920166\n",
      "After 500 batches, average loss/100 batches: 88.84007175445556\n",
      "After 600 batches, average loss/100 batches: 96.77255393981933\n",
      "After 700 batches, average loss/100 batches: 90.23875659942627\n",
      "After 800 batches, average loss/100 batches: 93.17143798828126\n",
      "After 900 batches, average loss/100 batches: 91.61796878814697\n",
      "After 1000 batches, average loss/100 batches: 90.90889602661133\n",
      "After 1100 batches, average loss/100 batches: 92.37550830841064\n",
      "After 1200 batches, average loss/100 batches: 92.64443267822266\n",
      "After 1300 batches, average loss/100 batches: 94.283212890625\n",
      "After 1400 batches, average loss/100 batches: 89.95849227905273\n",
      "After 1500 batches, average loss/100 batches: 91.30130947113037\n",
      "After 1600 batches, average loss/100 batches: 94.1746887588501\n",
      "After 1700 batches, average loss/100 batches: 94.67438861846924\n",
      "After 1800 batches, average loss/100 batches: 90.72618061065674\n",
      "After 1900 batches, average loss/100 batches: 93.25956130981446\n",
      "After 2000 batches, average loss/100 batches: 91.67464771270753\n",
      "After 2100 batches, average loss/100 batches: 92.8983433151245\n",
      "After 2200 batches, average loss/100 batches: 92.19143714904786\n",
      "After 2300 batches, average loss/100 batches: 93.30483818054199\n",
      "After 2400 batches, average loss/100 batches: 91.68028106689454\n",
      "Epoch 27/47\n",
      "After 100 batches, average loss/100 batches: 94.20940086364746\n",
      "After 200 batches, average loss/100 batches: 89.74355255126953\n",
      "After 300 batches, average loss/100 batches: 93.70515117645263\n",
      "After 400 batches, average loss/100 batches: 92.35719440460205\n",
      "After 500 batches, average loss/100 batches: 94.13358005523682\n",
      "After 600 batches, average loss/100 batches: 92.15701847076416\n",
      "After 700 batches, average loss/100 batches: 91.06431617736817\n",
      "After 800 batches, average loss/100 batches: 94.5782459640503\n",
      "After 900 batches, average loss/100 batches: 92.41133579254151\n",
      "After 1000 batches, average loss/100 batches: 90.27181098937989\n",
      "After 1100 batches, average loss/100 batches: 91.0290683746338\n",
      "After 1200 batches, average loss/100 batches: 94.92013549804688\n",
      "After 1300 batches, average loss/100 batches: 93.30127571105957\n",
      "After 1400 batches, average loss/100 batches: 96.32461688995362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1500 batches, average loss/100 batches: 90.02308612823487\n",
      "After 1600 batches, average loss/100 batches: 94.78154136657714\n",
      "After 1700 batches, average loss/100 batches: 92.9025271987915\n",
      "After 1800 batches, average loss/100 batches: 94.16095630645752\n",
      "After 1900 batches, average loss/100 batches: 95.84950012207031\n",
      "After 2000 batches, average loss/100 batches: 92.99199123382569\n",
      "After 2100 batches, average loss/100 batches: 93.66306007385253\n",
      "After 2200 batches, average loss/100 batches: 86.89715316772461\n",
      "After 2300 batches, average loss/100 batches: 95.02153858184815\n",
      "After 2400 batches, average loss/100 batches: 94.5888879776001\n",
      "Epoch 28/47\n",
      "After 100 batches, average loss/100 batches: 92.80522686004639\n",
      "After 200 batches, average loss/100 batches: 89.45699008941651\n",
      "After 300 batches, average loss/100 batches: 90.83427902221679\n",
      "After 400 batches, average loss/100 batches: 93.64577423095703\n",
      "After 500 batches, average loss/100 batches: 94.5161665725708\n",
      "After 600 batches, average loss/100 batches: 91.22417259216309\n",
      "After 700 batches, average loss/100 batches: 91.82444087982178\n",
      "After 800 batches, average loss/100 batches: 92.73691871643067\n",
      "After 900 batches, average loss/100 batches: 88.43370056152344\n",
      "After 1000 batches, average loss/100 batches: 91.15049091339111\n",
      "After 1100 batches, average loss/100 batches: 88.68271453857422\n",
      "After 1200 batches, average loss/100 batches: 94.55804069519043\n",
      "After 1300 batches, average loss/100 batches: 91.05982349395752\n",
      "After 1400 batches, average loss/100 batches: 92.4277815246582\n",
      "After 1500 batches, average loss/100 batches: 94.26474636077882\n",
      "After 1600 batches, average loss/100 batches: 91.48847263336182\n",
      "After 1700 batches, average loss/100 batches: 92.92860565185546\n",
      "After 1800 batches, average loss/100 batches: 91.47563220977783\n",
      "After 1900 batches, average loss/100 batches: 95.96780765533447\n",
      "After 2000 batches, average loss/100 batches: 93.50636215209961\n",
      "After 2100 batches, average loss/100 batches: 86.26108898162842\n",
      "After 2200 batches, average loss/100 batches: 91.22217685699462\n",
      "After 2300 batches, average loss/100 batches: 95.06255851745605\n",
      "After 2400 batches, average loss/100 batches: 91.42848823547364\n",
      "Epoch 29/47\n",
      "After 100 batches, average loss/100 batches: 96.73445819854736\n",
      "After 200 batches, average loss/100 batches: 87.61618217468262\n",
      "After 300 batches, average loss/100 batches: 90.43747955322266\n",
      "After 400 batches, average loss/100 batches: 92.17281661987305\n",
      "After 500 batches, average loss/100 batches: 88.17930721282958\n",
      "After 600 batches, average loss/100 batches: 87.99280521392822\n",
      "After 700 batches, average loss/100 batches: 91.37078300476074\n",
      "After 800 batches, average loss/100 batches: 90.95749126434326\n",
      "After 900 batches, average loss/100 batches: 93.5043830871582\n",
      "After 1000 batches, average loss/100 batches: 93.18899147033692\n",
      "After 1100 batches, average loss/100 batches: 89.64816761016846\n",
      "After 1200 batches, average loss/100 batches: 90.94028804779053\n",
      "After 1300 batches, average loss/100 batches: 90.51485664367675\n",
      "After 1400 batches, average loss/100 batches: 91.93790733337403\n",
      "After 1500 batches, average loss/100 batches: 90.91503253936767\n",
      "After 1600 batches, average loss/100 batches: 87.96884544372558\n",
      "After 1700 batches, average loss/100 batches: 94.1846212387085\n",
      "After 1800 batches, average loss/100 batches: 92.7565474319458\n",
      "After 1900 batches, average loss/100 batches: 92.98239418029785\n",
      "After 2000 batches, average loss/100 batches: 91.05056983947753\n",
      "After 2100 batches, average loss/100 batches: 90.79411972045898\n",
      "After 2200 batches, average loss/100 batches: 94.83376083374023\n",
      "After 2300 batches, average loss/100 batches: 95.33891151428223\n",
      "After 2400 batches, average loss/100 batches: 95.70069202423096\n",
      "Epoch 30/47\n",
      "After 100 batches, average loss/100 batches: 91.87547245025635\n",
      "After 200 batches, average loss/100 batches: 89.83768203735352\n",
      "After 300 batches, average loss/100 batches: 91.11160060882568\n",
      "After 400 batches, average loss/100 batches: 86.95029716491699\n",
      "After 500 batches, average loss/100 batches: 94.80831077575684\n",
      "After 600 batches, average loss/100 batches: 89.82479480743409\n",
      "After 700 batches, average loss/100 batches: 89.13407806396485\n",
      "After 800 batches, average loss/100 batches: 89.74630073547364\n",
      "After 900 batches, average loss/100 batches: 91.63420715332032\n",
      "After 1000 batches, average loss/100 batches: 93.98960922241211\n",
      "After 1100 batches, average loss/100 batches: 94.14939491271973\n",
      "After 1200 batches, average loss/100 batches: 92.4271322631836\n",
      "After 1300 batches, average loss/100 batches: 86.8339147567749\n",
      "After 1400 batches, average loss/100 batches: 90.97341407775879\n",
      "After 1500 batches, average loss/100 batches: 91.31024116516113\n",
      "After 1600 batches, average loss/100 batches: 87.85662158966065\n",
      "After 1700 batches, average loss/100 batches: 92.44009159088135\n",
      "After 1800 batches, average loss/100 batches: 93.94687450408935\n",
      "After 1900 batches, average loss/100 batches: 92.58376716613769\n",
      "After 2000 batches, average loss/100 batches: 93.12602767944335\n",
      "After 2100 batches, average loss/100 batches: 90.63549522399903\n",
      "After 2200 batches, average loss/100 batches: 91.65825424194335\n",
      "After 2300 batches, average loss/100 batches: 84.14265209197998\n",
      "After 2400 batches, average loss/100 batches: 87.9656851196289\n",
      "Epoch 31/47\n",
      "After 100 batches, average loss/100 batches: 93.38636554718018\n",
      "After 200 batches, average loss/100 batches: 90.26608116149902\n",
      "After 300 batches, average loss/100 batches: 90.78344032287598\n",
      "After 400 batches, average loss/100 batches: 87.44903007507324\n",
      "After 500 batches, average loss/100 batches: 93.5809101486206\n",
      "After 600 batches, average loss/100 batches: 90.59622035980225\n",
      "After 700 batches, average loss/100 batches: 85.39452495574952\n",
      "After 800 batches, average loss/100 batches: 96.19040756225586\n",
      "After 900 batches, average loss/100 batches: 91.54780708312988\n",
      "After 1000 batches, average loss/100 batches: 91.63137111663818\n",
      "After 1100 batches, average loss/100 batches: 92.16120471954346\n",
      "After 1200 batches, average loss/100 batches: 93.32873760223389\n",
      "After 1300 batches, average loss/100 batches: 92.69541511535644\n",
      "After 1400 batches, average loss/100 batches: 93.09180816650391\n",
      "After 1500 batches, average loss/100 batches: 97.06428405761719\n",
      "After 1600 batches, average loss/100 batches: 88.14876327514648\n",
      "After 1700 batches, average loss/100 batches: 92.08455989837647\n",
      "After 1800 batches, average loss/100 batches: 86.15365280151367\n",
      "After 1900 batches, average loss/100 batches: 96.17148338317871\n",
      "After 2000 batches, average loss/100 batches: 90.43369987487793\n",
      "After 2100 batches, average loss/100 batches: 88.77664756774902\n",
      "After 2200 batches, average loss/100 batches: 95.2080449295044\n",
      "After 2300 batches, average loss/100 batches: 90.48269886016845\n",
      "After 2400 batches, average loss/100 batches: 88.9506468963623\n",
      "Epoch 32/47\n",
      "After 100 batches, average loss/100 batches: 92.78261810302735\n",
      "After 200 batches, average loss/100 batches: 92.79593120574951\n",
      "After 300 batches, average loss/100 batches: 89.55341640472412\n",
      "After 400 batches, average loss/100 batches: 87.94312984466552\n",
      "After 500 batches, average loss/100 batches: 89.38828792572022\n",
      "After 600 batches, average loss/100 batches: 92.76747447967529\n",
      "After 700 batches, average loss/100 batches: 90.98936260223388\n",
      "After 800 batches, average loss/100 batches: 89.6820442199707\n",
      "After 900 batches, average loss/100 batches: 86.8048267364502\n",
      "After 1000 batches, average loss/100 batches: 91.36392463684082\n",
      "After 1100 batches, average loss/100 batches: 87.40413875579834\n",
      "After 1200 batches, average loss/100 batches: 90.49831525802612\n",
      "After 1300 batches, average loss/100 batches: 91.88355419158935\n",
      "After 1400 batches, average loss/100 batches: 89.1333215713501\n",
      "After 1500 batches, average loss/100 batches: 89.46041114807129\n",
      "After 1600 batches, average loss/100 batches: 89.98585006713867\n",
      "After 1700 batches, average loss/100 batches: 88.49038272857666\n",
      "After 1800 batches, average loss/100 batches: 90.58521892547607\n",
      "After 1900 batches, average loss/100 batches: 89.69497287750244\n",
      "After 2000 batches, average loss/100 batches: 93.48278228759766\n",
      "After 2100 batches, average loss/100 batches: 90.73067611694336\n",
      "After 2200 batches, average loss/100 batches: 88.999408493042\n",
      "After 2300 batches, average loss/100 batches: 86.69629306793213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2400 batches, average loss/100 batches: 89.98759967803954\n",
      "Epoch 33/47\n",
      "After 100 batches, average loss/100 batches: 93.66020877838135\n",
      "After 200 batches, average loss/100 batches: 90.33451354980468\n",
      "After 300 batches, average loss/100 batches: 89.86055465698242\n",
      "After 400 batches, average loss/100 batches: 92.28289386749267\n",
      "After 500 batches, average loss/100 batches: 92.06903747558594\n",
      "After 600 batches, average loss/100 batches: 87.64637699127198\n",
      "After 700 batches, average loss/100 batches: 92.55069244384765\n",
      "After 800 batches, average loss/100 batches: 86.9928115081787\n",
      "After 900 batches, average loss/100 batches: 87.10671592712403\n",
      "After 1000 batches, average loss/100 batches: 90.50275707244873\n",
      "After 1100 batches, average loss/100 batches: 93.18638568878174\n",
      "After 1200 batches, average loss/100 batches: 94.34265716552734\n",
      "After 1300 batches, average loss/100 batches: 86.71833976745606\n",
      "After 1400 batches, average loss/100 batches: 95.66843772888184\n",
      "After 1500 batches, average loss/100 batches: 91.19966484069825\n",
      "After 1600 batches, average loss/100 batches: 92.70332061767579\n",
      "After 1700 batches, average loss/100 batches: 91.06848930358886\n",
      "After 1800 batches, average loss/100 batches: 93.0971195602417\n",
      "After 1900 batches, average loss/100 batches: 88.87219890594483\n",
      "After 2000 batches, average loss/100 batches: 91.37735919952392\n",
      "After 2100 batches, average loss/100 batches: 89.54520442962647\n",
      "After 2200 batches, average loss/100 batches: 92.56177856445312\n",
      "After 2300 batches, average loss/100 batches: 94.71972648620606\n",
      "After 2400 batches, average loss/100 batches: 88.18226703643799\n",
      "Epoch 34/47\n",
      "After 100 batches, average loss/100 batches: 91.9386792755127\n",
      "After 200 batches, average loss/100 batches: 92.0470320892334\n",
      "After 300 batches, average loss/100 batches: 88.19452709197998\n",
      "After 400 batches, average loss/100 batches: 89.54083499908447\n",
      "After 500 batches, average loss/100 batches: 90.24550281524658\n",
      "After 600 batches, average loss/100 batches: 94.83830059051513\n",
      "After 700 batches, average loss/100 batches: 90.36180465698243\n",
      "After 800 batches, average loss/100 batches: 91.38736419677734\n",
      "After 900 batches, average loss/100 batches: 89.06309803009033\n",
      "After 1000 batches, average loss/100 batches: 89.17273300170899\n",
      "After 1100 batches, average loss/100 batches: 92.3508374595642\n",
      "After 1200 batches, average loss/100 batches: 86.36455097198487\n",
      "After 1300 batches, average loss/100 batches: 90.89234855651856\n",
      "After 1400 batches, average loss/100 batches: 88.13667064666748\n",
      "After 1500 batches, average loss/100 batches: 90.35438129425049\n",
      "After 1600 batches, average loss/100 batches: 89.78861003875733\n",
      "After 1700 batches, average loss/100 batches: 87.90609874725342\n",
      "After 1800 batches, average loss/100 batches: 88.83992321014404\n",
      "After 1900 batches, average loss/100 batches: 89.0756212234497\n",
      "After 2000 batches, average loss/100 batches: 93.40866458892822\n",
      "After 2100 batches, average loss/100 batches: 90.5774026107788\n",
      "After 2200 batches, average loss/100 batches: 91.0134177017212\n",
      "After 2300 batches, average loss/100 batches: 93.77956745147705\n",
      "After 2400 batches, average loss/100 batches: 89.48977321624756\n",
      "Epoch 35/47\n",
      "After 100 batches, average loss/100 batches: 86.71196235656738\n",
      "After 200 batches, average loss/100 batches: 88.72054359436035\n",
      "After 300 batches, average loss/100 batches: 88.48574325561523\n",
      "After 400 batches, average loss/100 batches: 89.51613857269287\n",
      "After 500 batches, average loss/100 batches: 88.44970397949218\n",
      "After 600 batches, average loss/100 batches: 90.54257545471191\n",
      "After 700 batches, average loss/100 batches: 89.20453289031983\n",
      "After 800 batches, average loss/100 batches: 94.50419849395752\n",
      "After 900 batches, average loss/100 batches: 89.80854866027832\n",
      "After 1000 batches, average loss/100 batches: 89.16904006958008\n",
      "After 1100 batches, average loss/100 batches: 94.62626129150391\n",
      "After 1200 batches, average loss/100 batches: 91.30865795135497\n",
      "After 1300 batches, average loss/100 batches: 92.69364986419677\n",
      "After 1400 batches, average loss/100 batches: 86.5267805480957\n",
      "After 1500 batches, average loss/100 batches: 87.49537548065186\n",
      "After 1600 batches, average loss/100 batches: 89.25905834197998\n",
      "After 1700 batches, average loss/100 batches: 89.92727630615235\n",
      "After 1800 batches, average loss/100 batches: 91.46818145751953\n",
      "After 1900 batches, average loss/100 batches: 95.52904697418212\n",
      "After 2000 batches, average loss/100 batches: 89.76424816131592\n",
      "After 2100 batches, average loss/100 batches: 89.9163911819458\n",
      "After 2200 batches, average loss/100 batches: 90.85194557189942\n",
      "After 2300 batches, average loss/100 batches: 92.74339698791503\n",
      "After 2400 batches, average loss/100 batches: 84.79068958282471\n",
      "Epoch 36/47\n",
      "After 100 batches, average loss/100 batches: 91.83276111602783\n",
      "After 200 batches, average loss/100 batches: 88.9747539138794\n",
      "After 300 batches, average loss/100 batches: 84.6512714767456\n",
      "After 400 batches, average loss/100 batches: 83.44847766876221\n",
      "After 500 batches, average loss/100 batches: 88.57948196411132\n",
      "After 600 batches, average loss/100 batches: 89.40878078460693\n",
      "After 700 batches, average loss/100 batches: 90.56324256896973\n",
      "After 800 batches, average loss/100 batches: 89.42651805877685\n",
      "After 900 batches, average loss/100 batches: 88.62961849212647\n",
      "After 1000 batches, average loss/100 batches: 88.11572513580322\n",
      "After 1100 batches, average loss/100 batches: 90.67424465179444\n",
      "After 1200 batches, average loss/100 batches: 86.79159465789795\n",
      "After 1300 batches, average loss/100 batches: 86.25775489807128\n",
      "After 1400 batches, average loss/100 batches: 85.02476009368897\n",
      "After 1500 batches, average loss/100 batches: 93.03794288635254\n",
      "After 1600 batches, average loss/100 batches: 89.65473987579345\n",
      "After 1700 batches, average loss/100 batches: 84.18656909942626\n",
      "After 1800 batches, average loss/100 batches: 90.214541015625\n",
      "After 1900 batches, average loss/100 batches: 86.20398975372315\n",
      "After 2000 batches, average loss/100 batches: 89.79076793670654\n",
      "After 2100 batches, average loss/100 batches: 90.62265274047851\n",
      "After 2200 batches, average loss/100 batches: 86.42704742431641\n",
      "After 2300 batches, average loss/100 batches: 88.60146865844726\n",
      "After 2400 batches, average loss/100 batches: 91.00766719818115\n",
      "Epoch 37/47\n",
      "After 100 batches, average loss/100 batches: 87.42764091491699\n",
      "After 200 batches, average loss/100 batches: 91.35848476409912\n",
      "After 300 batches, average loss/100 batches: 91.39554954528809\n",
      "After 400 batches, average loss/100 batches: 89.25918445587158\n",
      "After 500 batches, average loss/100 batches: 93.2710336303711\n",
      "After 600 batches, average loss/100 batches: 90.37901653289795\n",
      "After 700 batches, average loss/100 batches: 88.36960823059081\n",
      "After 800 batches, average loss/100 batches: 87.95739891052246\n",
      "After 900 batches, average loss/100 batches: 86.27353507995605\n",
      "After 1000 batches, average loss/100 batches: 85.83525005340576\n",
      "After 1100 batches, average loss/100 batches: 89.41642299652099\n",
      "After 1200 batches, average loss/100 batches: 86.38250030517578\n",
      "After 1300 batches, average loss/100 batches: 86.47254768371582\n",
      "After 1400 batches, average loss/100 batches: 91.63573299407959\n",
      "After 1500 batches, average loss/100 batches: 81.78723041534424\n",
      "After 1600 batches, average loss/100 batches: 87.21111114501953\n",
      "After 1700 batches, average loss/100 batches: 87.15487823486328\n",
      "After 1800 batches, average loss/100 batches: 87.94971092224121\n",
      "After 1900 batches, average loss/100 batches: 90.67318214416504\n",
      "After 2000 batches, average loss/100 batches: 91.27352714538574\n",
      "After 2100 batches, average loss/100 batches: 90.72582836151123\n",
      "After 2200 batches, average loss/100 batches: 91.06094051361084\n",
      "After 2300 batches, average loss/100 batches: 89.6025617980957\n",
      "After 2400 batches, average loss/100 batches: 83.99716892242432\n",
      "Epoch 38/47\n",
      "After 100 batches, average loss/100 batches: 90.8233800125122\n",
      "After 200 batches, average loss/100 batches: 91.73125171661377\n",
      "After 300 batches, average loss/100 batches: 83.69703914642334\n",
      "After 400 batches, average loss/100 batches: 88.77460540771484\n",
      "After 500 batches, average loss/100 batches: 93.376725730896\n",
      "After 600 batches, average loss/100 batches: 89.94790622711182\n",
      "After 700 batches, average loss/100 batches: 83.83871841430664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 800 batches, average loss/100 batches: 85.45818172454834\n",
      "After 900 batches, average loss/100 batches: 89.47337585449219\n",
      "After 1000 batches, average loss/100 batches: 89.36895957946777\n",
      "After 1100 batches, average loss/100 batches: 86.01013772964478\n",
      "After 1200 batches, average loss/100 batches: 88.79148273468017\n",
      "After 1300 batches, average loss/100 batches: 87.70394046783447\n",
      "After 1400 batches, average loss/100 batches: 92.00237712860107\n",
      "After 1500 batches, average loss/100 batches: 88.15280971527099\n",
      "After 1600 batches, average loss/100 batches: 89.28662929534912\n",
      "After 1700 batches, average loss/100 batches: 88.20903285980225\n",
      "After 1800 batches, average loss/100 batches: 90.71666542053222\n",
      "After 1900 batches, average loss/100 batches: 89.38992572784424\n",
      "After 2000 batches, average loss/100 batches: 84.18445335388184\n",
      "After 2100 batches, average loss/100 batches: 85.53436954498291\n",
      "After 2200 batches, average loss/100 batches: 88.52389167785644\n",
      "After 2300 batches, average loss/100 batches: 89.54754859924316\n",
      "After 2400 batches, average loss/100 batches: 87.87916011810303\n",
      "Epoch 39/47\n",
      "After 100 batches, average loss/100 batches: 87.0618210220337\n",
      "After 200 batches, average loss/100 batches: 84.79046104431153\n",
      "After 300 batches, average loss/100 batches: 86.86247955322266\n",
      "After 400 batches, average loss/100 batches: 86.55674392700195\n",
      "After 500 batches, average loss/100 batches: 86.31557647705078\n",
      "After 600 batches, average loss/100 batches: 85.71286350250244\n",
      "After 700 batches, average loss/100 batches: 85.08073085784912\n",
      "After 800 batches, average loss/100 batches: 87.56155033111573\n",
      "After 900 batches, average loss/100 batches: 84.064465675354\n",
      "After 1000 batches, average loss/100 batches: 91.98976402282715\n",
      "After 1100 batches, average loss/100 batches: 92.6040714263916\n",
      "After 1200 batches, average loss/100 batches: 84.57753742218017\n",
      "After 1300 batches, average loss/100 batches: 86.47685955047608\n",
      "After 1400 batches, average loss/100 batches: 93.34945335388184\n",
      "After 1500 batches, average loss/100 batches: 88.70333957672119\n",
      "After 1600 batches, average loss/100 batches: 90.00980201721191\n",
      "After 1700 batches, average loss/100 batches: 89.57424911499024\n",
      "After 1800 batches, average loss/100 batches: 90.1779446029663\n",
      "After 1900 batches, average loss/100 batches: 87.07496147155761\n",
      "After 2000 batches, average loss/100 batches: 87.69196617126465\n",
      "After 2100 batches, average loss/100 batches: 92.5354443359375\n",
      "After 2200 batches, average loss/100 batches: 91.13719276428223\n",
      "After 2300 batches, average loss/100 batches: 90.75947723388671\n",
      "After 2400 batches, average loss/100 batches: 88.11107814788818\n",
      "Epoch 40/47\n",
      "After 100 batches, average loss/100 batches: 90.84053661346435\n",
      "After 200 batches, average loss/100 batches: 91.65415267944336\n",
      "After 300 batches, average loss/100 batches: 83.57380664825439\n",
      "After 400 batches, average loss/100 batches: 90.39867092132569\n",
      "After 500 batches, average loss/100 batches: 85.50551483154297\n",
      "After 600 batches, average loss/100 batches: 86.75893688201904\n",
      "After 700 batches, average loss/100 batches: 86.98138633728027\n",
      "After 800 batches, average loss/100 batches: 90.2759093093872\n",
      "After 900 batches, average loss/100 batches: 83.8052220916748\n",
      "After 1000 batches, average loss/100 batches: 87.3710799407959\n",
      "After 1100 batches, average loss/100 batches: 83.9132865524292\n",
      "After 1200 batches, average loss/100 batches: 83.68879638671875\n",
      "After 1300 batches, average loss/100 batches: 89.7919446182251\n",
      "After 1400 batches, average loss/100 batches: 87.76366443634033\n",
      "After 1500 batches, average loss/100 batches: 86.10922039031982\n",
      "After 1600 batches, average loss/100 batches: 89.1588946723938\n",
      "After 1700 batches, average loss/100 batches: 84.46083820343017\n",
      "After 1800 batches, average loss/100 batches: 88.04382781982422\n",
      "After 1900 batches, average loss/100 batches: 87.11259939193725\n",
      "After 2000 batches, average loss/100 batches: 88.18381195068359\n",
      "After 2100 batches, average loss/100 batches: 84.96522693634033\n",
      "After 2200 batches, average loss/100 batches: 87.24140277862548\n",
      "After 2300 batches, average loss/100 batches: 86.80854415893555\n",
      "After 2400 batches, average loss/100 batches: 90.70519515991211\n",
      "Epoch 41/47\n",
      "After 100 batches, average loss/100 batches: 92.70758209228515\n",
      "After 200 batches, average loss/100 batches: 88.37337459564209\n",
      "After 300 batches, average loss/100 batches: 87.78300327301025\n",
      "After 400 batches, average loss/100 batches: 91.92091079711913\n",
      "After 500 batches, average loss/100 batches: 82.4283636856079\n",
      "After 600 batches, average loss/100 batches: 88.01403453826904\n",
      "After 700 batches, average loss/100 batches: 83.92705673217773\n",
      "After 800 batches, average loss/100 batches: 90.04209293365479\n",
      "After 900 batches, average loss/100 batches: 93.26837326049805\n",
      "After 1000 batches, average loss/100 batches: 85.15373928070068\n",
      "After 1100 batches, average loss/100 batches: 89.58311656951905\n",
      "After 1200 batches, average loss/100 batches: 84.82625133514404\n",
      "After 1300 batches, average loss/100 batches: 88.85737434387207\n",
      "After 1400 batches, average loss/100 batches: 90.4878503036499\n",
      "After 1500 batches, average loss/100 batches: 86.56036083221436\n",
      "After 1600 batches, average loss/100 batches: 86.61511501312256\n",
      "After 1700 batches, average loss/100 batches: 88.68255771636963\n",
      "After 1800 batches, average loss/100 batches: 84.51420341491699\n",
      "After 1900 batches, average loss/100 batches: 89.00215049743652\n",
      "After 2000 batches, average loss/100 batches: 82.14460758209229\n",
      "After 2100 batches, average loss/100 batches: 88.4527746963501\n",
      "After 2200 batches, average loss/100 batches: 88.87684259414672\n",
      "After 2300 batches, average loss/100 batches: 88.66106872558593\n",
      "After 2400 batches, average loss/100 batches: 88.82731861114502\n",
      "Epoch 42/47\n",
      "After 100 batches, average loss/100 batches: 89.77532779693604\n",
      "After 200 batches, average loss/100 batches: 83.83880401611329\n",
      "After 300 batches, average loss/100 batches: 86.63880168914795\n",
      "After 400 batches, average loss/100 batches: 87.68228939056397\n",
      "After 500 batches, average loss/100 batches: 79.72999908447265\n",
      "After 600 batches, average loss/100 batches: 82.74722564697265\n",
      "After 700 batches, average loss/100 batches: 83.68337844848632\n",
      "After 800 batches, average loss/100 batches: 84.12263160705567\n",
      "After 900 batches, average loss/100 batches: 87.37369594573974\n",
      "After 1000 batches, average loss/100 batches: 85.47794261932373\n",
      "After 1100 batches, average loss/100 batches: 85.48650604248047\n",
      "After 1200 batches, average loss/100 batches: 87.61101440429688\n",
      "After 1300 batches, average loss/100 batches: 85.17373176574706\n",
      "After 1400 batches, average loss/100 batches: 86.70528007507325\n",
      "After 1500 batches, average loss/100 batches: 87.20404224395752\n",
      "After 1600 batches, average loss/100 batches: 88.11662925720215\n",
      "After 1700 batches, average loss/100 batches: 82.67555393218994\n",
      "After 1800 batches, average loss/100 batches: 87.92371433258057\n",
      "After 1900 batches, average loss/100 batches: 90.53303005218505\n",
      "After 2000 batches, average loss/100 batches: 90.67275588989258\n",
      "After 2100 batches, average loss/100 batches: 89.42345302581788\n",
      "After 2200 batches, average loss/100 batches: 89.70519805908204\n",
      "After 2300 batches, average loss/100 batches: 86.22666072845459\n",
      "After 2400 batches, average loss/100 batches: 90.24125911712646\n",
      "Epoch 43/47\n",
      "After 100 batches, average loss/100 batches: 87.77137218475342\n",
      "After 200 batches, average loss/100 batches: 95.61367912292481\n",
      "After 300 batches, average loss/100 batches: 87.13153533935547\n",
      "After 400 batches, average loss/100 batches: 85.9458279800415\n",
      "After 500 batches, average loss/100 batches: 88.30387771606445\n",
      "After 600 batches, average loss/100 batches: 88.02959442138672\n",
      "After 700 batches, average loss/100 batches: 90.01821670532226\n",
      "After 800 batches, average loss/100 batches: 86.48006061553956\n",
      "After 900 batches, average loss/100 batches: 87.84056247711182\n",
      "After 1000 batches, average loss/100 batches: 83.470873336792\n",
      "After 1100 batches, average loss/100 batches: 88.23264865875244\n",
      "After 1200 batches, average loss/100 batches: 87.75111339569092\n",
      "After 1300 batches, average loss/100 batches: 87.70182083129883\n",
      "After 1400 batches, average loss/100 batches: 85.8751770401001\n",
      "After 1500 batches, average loss/100 batches: 81.6481434249878\n",
      "After 1600 batches, average loss/100 batches: 83.87831245422363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1700 batches, average loss/100 batches: 88.75022712707519\n",
      "After 1800 batches, average loss/100 batches: 86.40903263092041\n",
      "After 1900 batches, average loss/100 batches: 90.07275394439698\n",
      "After 2000 batches, average loss/100 batches: 90.46919109344482\n",
      "After 2100 batches, average loss/100 batches: 89.3794552230835\n",
      "After 2200 batches, average loss/100 batches: 85.46105281829834\n",
      "After 2300 batches, average loss/100 batches: 85.61431381225586\n",
      "After 2400 batches, average loss/100 batches: 89.24904258728027\n",
      "Epoch 44/47\n",
      "After 100 batches, average loss/100 batches: 87.42599590301514\n",
      "After 200 batches, average loss/100 batches: 88.62701236724854\n",
      "After 300 batches, average loss/100 batches: 82.59353847503662\n",
      "After 400 batches, average loss/100 batches: 89.94264068603516\n",
      "After 500 batches, average loss/100 batches: 87.45118957519531\n",
      "After 600 batches, average loss/100 batches: 89.47705894470215\n",
      "After 700 batches, average loss/100 batches: 85.0923664855957\n",
      "After 800 batches, average loss/100 batches: 87.94015399932861\n",
      "After 900 batches, average loss/100 batches: 87.97736293792724\n",
      "After 1000 batches, average loss/100 batches: 83.84017623901367\n",
      "After 1100 batches, average loss/100 batches: 91.12506755828858\n",
      "After 1200 batches, average loss/100 batches: 84.12385639190674\n",
      "After 1300 batches, average loss/100 batches: 86.68308700561523\n",
      "After 1400 batches, average loss/100 batches: 89.92315444946288\n",
      "After 1500 batches, average loss/100 batches: 84.99751590728759\n",
      "After 1600 batches, average loss/100 batches: 82.93559497833252\n",
      "After 1700 batches, average loss/100 batches: 87.8489790725708\n",
      "After 1800 batches, average loss/100 batches: 90.61438831329346\n",
      "After 1900 batches, average loss/100 batches: 86.49014335632324\n",
      "After 2000 batches, average loss/100 batches: 86.07786060333252\n",
      "After 2100 batches, average loss/100 batches: 83.7365221786499\n",
      "After 2200 batches, average loss/100 batches: 84.39698760986329\n",
      "After 2300 batches, average loss/100 batches: 89.31646171569824\n",
      "After 2400 batches, average loss/100 batches: 84.39024906158447\n",
      "Epoch 45/47\n",
      "After 100 batches, average loss/100 batches: 90.33292881011963\n",
      "After 200 batches, average loss/100 batches: 82.54736164093018\n",
      "After 300 batches, average loss/100 batches: 85.04679649353028\n",
      "After 400 batches, average loss/100 batches: 87.85712844848632\n",
      "After 500 batches, average loss/100 batches: 88.53247203826905\n",
      "After 600 batches, average loss/100 batches: 87.16912754058838\n",
      "After 700 batches, average loss/100 batches: 85.10137588500976\n",
      "After 800 batches, average loss/100 batches: 86.59009273529053\n",
      "After 900 batches, average loss/100 batches: 86.67383506774902\n",
      "After 1000 batches, average loss/100 batches: 85.50563251495362\n",
      "After 1100 batches, average loss/100 batches: 84.47148818969727\n",
      "After 1200 batches, average loss/100 batches: 83.95660758972168\n",
      "After 1300 batches, average loss/100 batches: 86.55475555419922\n",
      "After 1400 batches, average loss/100 batches: 88.44247375488281\n",
      "After 1500 batches, average loss/100 batches: 86.55867961883546\n",
      "After 1600 batches, average loss/100 batches: 88.25697246551513\n",
      "After 1700 batches, average loss/100 batches: 83.07089786529541\n",
      "After 1800 batches, average loss/100 batches: 80.92128578186035\n",
      "After 1900 batches, average loss/100 batches: 85.34755138397217\n",
      "After 2000 batches, average loss/100 batches: 85.3952061843872\n",
      "After 2100 batches, average loss/100 batches: 86.33077777862549\n",
      "After 2200 batches, average loss/100 batches: 88.13400417327881\n",
      "After 2300 batches, average loss/100 batches: 85.88908519744874\n",
      "After 2400 batches, average loss/100 batches: 88.50734058380127\n",
      "Epoch 46/47\n",
      "After 100 batches, average loss/100 batches: 82.44782794952393\n",
      "After 200 batches, average loss/100 batches: 90.06819511413575\n",
      "After 300 batches, average loss/100 batches: 82.80000778198242\n",
      "After 400 batches, average loss/100 batches: 90.87905403137206\n",
      "After 500 batches, average loss/100 batches: 85.87978736877442\n",
      "After 600 batches, average loss/100 batches: 88.69058795928954\n",
      "After 700 batches, average loss/100 batches: 85.81637920379639\n",
      "After 800 batches, average loss/100 batches: 84.23321231842041\n",
      "After 900 batches, average loss/100 batches: 88.3802753829956\n",
      "After 1000 batches, average loss/100 batches: 86.65684917449951\n",
      "After 1100 batches, average loss/100 batches: 82.91302295684814\n",
      "After 1200 batches, average loss/100 batches: 87.60347869873047\n",
      "After 1300 batches, average loss/100 batches: 87.32064655303955\n",
      "After 1400 batches, average loss/100 batches: 88.16369079589843\n",
      "After 1500 batches, average loss/100 batches: 83.99900642395019\n",
      "After 1600 batches, average loss/100 batches: 86.5522978591919\n",
      "After 1700 batches, average loss/100 batches: 82.19990058898925\n",
      "After 1800 batches, average loss/100 batches: 86.14693260192871\n",
      "After 1900 batches, average loss/100 batches: 83.47836112976074\n",
      "After 2000 batches, average loss/100 batches: 84.84973478317261\n",
      "After 2100 batches, average loss/100 batches: 91.5299154663086\n",
      "After 2200 batches, average loss/100 batches: 85.87912025451661\n",
      "After 2300 batches, average loss/100 batches: 89.78161262512207\n",
      "After 2400 batches, average loss/100 batches: 86.38981903076171\n",
      "Epoch 47/47\n",
      "After 100 batches, average loss/100 batches: 85.17515670776368\n",
      "After 200 batches, average loss/100 batches: 84.11547550201416\n",
      "After 300 batches, average loss/100 batches: 84.49789669036865\n",
      "After 400 batches, average loss/100 batches: 76.8730457687378\n",
      "After 500 batches, average loss/100 batches: 84.07465755462647\n",
      "After 600 batches, average loss/100 batches: 84.97753955841064\n",
      "After 700 batches, average loss/100 batches: 84.65751411437988\n",
      "After 800 batches, average loss/100 batches: 83.75652988433838\n",
      "After 900 batches, average loss/100 batches: 85.08548816680909\n",
      "After 1000 batches, average loss/100 batches: 81.01873435974122\n",
      "After 1100 batches, average loss/100 batches: 84.30787162780761\n",
      "After 1200 batches, average loss/100 batches: 84.50987827301026\n",
      "After 1300 batches, average loss/100 batches: 88.7275085067749\n",
      "After 1400 batches, average loss/100 batches: 86.67063495635986\n",
      "After 1500 batches, average loss/100 batches: 84.4981510925293\n",
      "After 1600 batches, average loss/100 batches: 87.18915054321289\n",
      "After 1700 batches, average loss/100 batches: 86.04955207824707\n",
      "After 1800 batches, average loss/100 batches: 86.49471242904663\n",
      "After 1900 batches, average loss/100 batches: 90.52195442199707\n",
      "After 2000 batches, average loss/100 batches: 85.94465854644776\n",
      "After 2100 batches, average loss/100 batches: 89.63183948516846\n",
      "After 2200 batches, average loss/100 batches: 78.60001277923584\n",
      "After 2300 batches, average loss/100 batches: 82.54712909698486\n",
      "After 2400 batches, average loss/100 batches: 88.68804248809815\n"
     ]
    }
   ],
   "source": [
    "# For dataset 1, models were trained for 3 epochs\n",
    "# For dataset 2, models were trained for 50 epochs\n",
    "print('Training GRU based network.')\n",
    "learning_rate = 0.0001\n",
    "encoder_gru.train() # Set model to training mode\n",
    "decoder_gru.train() # Set model to training mode\n",
    "\n",
    "gru_losses = trainIters(encoder_gru, decoder_gru, dataloader, epochs=47, learning_rate = learning_rate)\n",
    "np.save('gru2_losses.npy', gru_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "After 100 batches, average loss/100 batches: 83.25332092285156\n",
      "After 200 batches, average loss/100 batches: 77.87617252349854\n",
      "After 300 batches, average loss/100 batches: 80.29739896774292\n",
      "After 400 batches, average loss/100 batches: 84.31723602294922\n",
      "After 500 batches, average loss/100 batches: 82.50146308898925\n",
      "After 600 batches, average loss/100 batches: 83.69223701477051\n",
      "After 700 batches, average loss/100 batches: 84.5814856338501\n",
      "After 800 batches, average loss/100 batches: 83.75546901702882\n",
      "After 900 batches, average loss/100 batches: 80.55284404754639\n",
      "After 1000 batches, average loss/100 batches: 82.66737648010253\n",
      "After 1100 batches, average loss/100 batches: 84.23172630310059\n",
      "After 1200 batches, average loss/100 batches: 79.9211441040039\n",
      "After 1300 batches, average loss/100 batches: 80.58261238098144\n",
      "After 1400 batches, average loss/100 batches: 83.7788981628418\n",
      "After 1500 batches, average loss/100 batches: 78.10280094146728\n",
      "After 1600 batches, average loss/100 batches: 83.97566070556641\n",
      "After 1700 batches, average loss/100 batches: 83.38629360198975\n",
      "After 1800 batches, average loss/100 batches: 81.85943439483643\n",
      "After 1900 batches, average loss/100 batches: 84.68093410491943\n",
      "After 2000 batches, average loss/100 batches: 83.96903553009034\n",
      "After 2100 batches, average loss/100 batches: 81.28284244537353\n",
      "After 2200 batches, average loss/100 batches: 85.20353176116943\n",
      "After 2300 batches, average loss/100 batches: 80.61365581512452\n",
      "After 2400 batches, average loss/100 batches: 82.14305862426758\n",
      "Epoch 2/25\n",
      "After 100 batches, average loss/100 batches: 85.24061107635498\n",
      "After 200 batches, average loss/100 batches: 74.69401153564453\n",
      "After 300 batches, average loss/100 batches: 82.01332611083984\n",
      "After 400 batches, average loss/100 batches: 80.41541347503662\n",
      "After 500 batches, average loss/100 batches: 81.71151268005372\n",
      "After 600 batches, average loss/100 batches: 81.1405564880371\n",
      "After 700 batches, average loss/100 batches: 82.07425357818603\n",
      "After 800 batches, average loss/100 batches: 79.10503273010254\n",
      "After 900 batches, average loss/100 batches: 80.75635005950927\n",
      "After 1000 batches, average loss/100 batches: 85.5266711807251\n",
      "After 1100 batches, average loss/100 batches: 79.7753345489502\n",
      "After 1200 batches, average loss/100 batches: 82.44759071350097\n",
      "After 1300 batches, average loss/100 batches: 84.64295742034912\n",
      "After 1400 batches, average loss/100 batches: 82.01695777893066\n",
      "After 1500 batches, average loss/100 batches: 83.39913120269776\n",
      "After 1600 batches, average loss/100 batches: 83.19188663482666\n",
      "After 1700 batches, average loss/100 batches: 80.69552364349366\n",
      "After 1800 batches, average loss/100 batches: 80.57637725830078\n",
      "After 1900 batches, average loss/100 batches: 79.67267311096191\n",
      "After 2000 batches, average loss/100 batches: 82.54333602905274\n",
      "After 2100 batches, average loss/100 batches: 85.14701103210449\n",
      "After 2200 batches, average loss/100 batches: 87.48289054870605\n",
      "After 2300 batches, average loss/100 batches: 77.64916217803955\n",
      "After 2400 batches, average loss/100 batches: 80.02741878509522\n",
      "Epoch 3/25\n",
      "After 100 batches, average loss/100 batches: 82.8374970626831\n",
      "After 200 batches, average loss/100 batches: 84.49014450073243\n",
      "After 300 batches, average loss/100 batches: 79.63498817443848\n",
      "After 400 batches, average loss/100 batches: 79.137216796875\n",
      "After 500 batches, average loss/100 batches: 81.20682285308838\n",
      "After 600 batches, average loss/100 batches: 80.09822742462158\n",
      "After 700 batches, average loss/100 batches: 81.0167790222168\n",
      "After 800 batches, average loss/100 batches: 84.0952063369751\n",
      "After 900 batches, average loss/100 batches: 81.4597481918335\n",
      "After 1000 batches, average loss/100 batches: 78.66448638916016\n",
      "After 1100 batches, average loss/100 batches: 81.0488345336914\n",
      "After 1200 batches, average loss/100 batches: 84.96343368530273\n",
      "After 1300 batches, average loss/100 batches: 82.10747421264648\n",
      "After 1400 batches, average loss/100 batches: 82.13083946228028\n",
      "After 1500 batches, average loss/100 batches: 82.87056854248047\n",
      "After 1600 batches, average loss/100 batches: 86.27732555389404\n",
      "After 1700 batches, average loss/100 batches: 84.69310546875\n",
      "After 1800 batches, average loss/100 batches: 84.28932510375977\n",
      "After 1900 batches, average loss/100 batches: 78.84834720611572\n",
      "After 2000 batches, average loss/100 batches: 82.12592727661132\n",
      "After 2100 batches, average loss/100 batches: 81.7005823135376\n",
      "After 2200 batches, average loss/100 batches: 80.29365188598632\n",
      "After 2300 batches, average loss/100 batches: 78.6975350189209\n",
      "After 2400 batches, average loss/100 batches: 82.95568767547607\n",
      "Epoch 4/25\n",
      "After 100 batches, average loss/100 batches: 84.31030197143555\n",
      "After 200 batches, average loss/100 batches: 81.90292625427246\n",
      "After 300 batches, average loss/100 batches: 86.53083187103272\n",
      "After 400 batches, average loss/100 batches: 79.66505546569825\n",
      "After 500 batches, average loss/100 batches: 79.78393737792969\n",
      "After 600 batches, average loss/100 batches: 81.82697662353516\n",
      "After 700 batches, average loss/100 batches: 80.28734212875366\n",
      "After 800 batches, average loss/100 batches: 84.69117832183838\n",
      "After 900 batches, average loss/100 batches: 83.96585441589356\n",
      "After 1000 batches, average loss/100 batches: 81.6492469406128\n",
      "After 1100 batches, average loss/100 batches: 81.21213054656982\n",
      "After 1200 batches, average loss/100 batches: 81.31778717041016\n",
      "After 1300 batches, average loss/100 batches: 85.26158372879028\n",
      "After 1400 batches, average loss/100 batches: 80.9295429611206\n",
      "After 1500 batches, average loss/100 batches: 79.19413131713867\n",
      "After 1600 batches, average loss/100 batches: 83.52825336456299\n",
      "After 1700 batches, average loss/100 batches: 74.7650089263916\n",
      "After 1800 batches, average loss/100 batches: 78.37280097961425\n",
      "After 1900 batches, average loss/100 batches: 78.09761497497558\n",
      "After 2000 batches, average loss/100 batches: 80.64869766235351\n",
      "After 2100 batches, average loss/100 batches: 78.72898616790772\n",
      "After 2200 batches, average loss/100 batches: 82.20730228424073\n",
      "After 2300 batches, average loss/100 batches: 80.49527877807617\n",
      "After 2400 batches, average loss/100 batches: 83.7484591293335\n",
      "Epoch 5/25\n",
      "After 100 batches, average loss/100 batches: 80.48500659942627\n",
      "After 200 batches, average loss/100 batches: 75.21868228912354\n",
      "After 300 batches, average loss/100 batches: 84.13084487915039\n",
      "After 400 batches, average loss/100 batches: 83.32021255493164\n",
      "After 500 batches, average loss/100 batches: 79.74393863677979\n",
      "After 600 batches, average loss/100 batches: 80.97334125518799\n",
      "After 700 batches, average loss/100 batches: 79.74169256210327\n",
      "After 800 batches, average loss/100 batches: 82.45362648010254\n",
      "After 900 batches, average loss/100 batches: 81.943309135437\n",
      "After 1000 batches, average loss/100 batches: 84.79087158203124\n",
      "After 1100 batches, average loss/100 batches: 81.85578075408935\n",
      "After 1200 batches, average loss/100 batches: 78.81502304077148\n",
      "After 1300 batches, average loss/100 batches: 81.81102565765381\n",
      "After 1400 batches, average loss/100 batches: 82.15833679199218\n",
      "After 1500 batches, average loss/100 batches: 82.50170623779297\n",
      "After 1600 batches, average loss/100 batches: 78.72101341247559\n",
      "After 1700 batches, average loss/100 batches: 78.80195976257325\n",
      "After 1800 batches, average loss/100 batches: 79.55568027496338\n",
      "After 1900 batches, average loss/100 batches: 83.03670070648194\n",
      "After 2000 batches, average loss/100 batches: 82.10126041412353\n",
      "After 2100 batches, average loss/100 batches: 82.72865589141846\n",
      "After 2200 batches, average loss/100 batches: 83.63073379516601\n",
      "After 2300 batches, average loss/100 batches: 79.98768758773804\n",
      "After 2400 batches, average loss/100 batches: 79.42291687011719\n",
      "Epoch 6/25\n",
      "After 100 batches, average loss/100 batches: 81.77850549697877\n",
      "After 200 batches, average loss/100 batches: 82.98899665832519\n",
      "After 300 batches, average loss/100 batches: 81.70311595916748\n",
      "After 400 batches, average loss/100 batches: 82.33314056396485\n",
      "After 500 batches, average loss/100 batches: 82.91126361846923\n",
      "After 600 batches, average loss/100 batches: 81.78863330841064\n",
      "After 700 batches, average loss/100 batches: 80.09714851379394\n",
      "After 800 batches, average loss/100 batches: 83.06838851928711\n",
      "After 900 batches, average loss/100 batches: 81.33462493896485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1000 batches, average loss/100 batches: 78.06983642578125\n",
      "After 1100 batches, average loss/100 batches: 85.19825824737549\n",
      "After 1200 batches, average loss/100 batches: 79.49597217559814\n",
      "After 1300 batches, average loss/100 batches: 79.69749286651611\n",
      "After 1400 batches, average loss/100 batches: 82.00688556671143\n",
      "After 1500 batches, average loss/100 batches: 83.80936943054199\n",
      "After 1600 batches, average loss/100 batches: 79.35424644470214\n",
      "After 1700 batches, average loss/100 batches: 80.48421611785889\n",
      "After 1800 batches, average loss/100 batches: 80.40604885101318\n",
      "After 1900 batches, average loss/100 batches: 85.70776134490967\n",
      "After 2000 batches, average loss/100 batches: 82.1398987197876\n",
      "After 2100 batches, average loss/100 batches: 81.08880558013917\n",
      "After 2200 batches, average loss/100 batches: 83.3204210281372\n",
      "After 2300 batches, average loss/100 batches: 81.63839599609375\n",
      "After 2400 batches, average loss/100 batches: 77.04505462646485\n",
      "Epoch 7/25\n",
      "After 100 batches, average loss/100 batches: 80.26991661071777\n",
      "After 200 batches, average loss/100 batches: 82.94635486602783\n",
      "After 300 batches, average loss/100 batches: 73.72140874862671\n",
      "After 400 batches, average loss/100 batches: 79.53957481384278\n",
      "After 500 batches, average loss/100 batches: 79.98721725463867\n",
      "After 600 batches, average loss/100 batches: 81.21607706069946\n",
      "After 700 batches, average loss/100 batches: 83.70134433746338\n",
      "After 800 batches, average loss/100 batches: 80.96068313598633\n",
      "After 900 batches, average loss/100 batches: 78.48052955627442\n",
      "After 1000 batches, average loss/100 batches: 80.87926036834716\n",
      "After 1100 batches, average loss/100 batches: 83.65253520965577\n",
      "After 1200 batches, average loss/100 batches: 85.34974845886231\n",
      "After 1300 batches, average loss/100 batches: 82.38252674102783\n",
      "After 1400 batches, average loss/100 batches: 83.63120491027831\n",
      "After 1500 batches, average loss/100 batches: 78.74082401275635\n",
      "After 1600 batches, average loss/100 batches: 78.96789257049561\n",
      "After 1700 batches, average loss/100 batches: 85.07039421081544\n",
      "After 1800 batches, average loss/100 batches: 80.8510209274292\n",
      "After 1900 batches, average loss/100 batches: 78.89458320617676\n",
      "After 2000 batches, average loss/100 batches: 85.55817295074463\n",
      "After 2100 batches, average loss/100 batches: 85.01385776519776\n",
      "After 2200 batches, average loss/100 batches: 79.13713809967041\n",
      "After 2300 batches, average loss/100 batches: 83.69640361785889\n",
      "After 2400 batches, average loss/100 batches: 80.51008583068848\n",
      "Epoch 8/25\n",
      "After 100 batches, average loss/100 batches: 87.57463150024414\n",
      "After 200 batches, average loss/100 batches: 77.76620983123779\n",
      "After 300 batches, average loss/100 batches: 80.74165607452393\n",
      "After 400 batches, average loss/100 batches: 80.48679691314697\n",
      "After 500 batches, average loss/100 batches: 81.94359573364258\n",
      "After 600 batches, average loss/100 batches: 79.06022281646729\n",
      "After 700 batches, average loss/100 batches: 77.97564025878906\n",
      "After 800 batches, average loss/100 batches: 81.61636779785157\n",
      "After 900 batches, average loss/100 batches: 78.77662490844726\n",
      "After 1000 batches, average loss/100 batches: 82.21162662506103\n",
      "After 1100 batches, average loss/100 batches: 80.37362785339356\n",
      "After 1200 batches, average loss/100 batches: 81.78414337158203\n",
      "After 1300 batches, average loss/100 batches: 78.93117408752441\n",
      "After 1400 batches, average loss/100 batches: 76.9502357673645\n",
      "After 1500 batches, average loss/100 batches: 80.21259199142456\n",
      "After 1600 batches, average loss/100 batches: 80.97117874145508\n",
      "After 1700 batches, average loss/100 batches: 82.6293588256836\n",
      "After 1800 batches, average loss/100 batches: 83.7120682144165\n",
      "After 1900 batches, average loss/100 batches: 76.94789604187012\n",
      "After 2000 batches, average loss/100 batches: 80.56070026397705\n",
      "After 2100 batches, average loss/100 batches: 78.88018905639649\n",
      "After 2200 batches, average loss/100 batches: 82.51851245880127\n",
      "After 2300 batches, average loss/100 batches: 81.68449298858643\n",
      "After 2400 batches, average loss/100 batches: 84.6248764038086\n",
      "Epoch 9/25\n",
      "After 100 batches, average loss/100 batches: 81.15778816223144\n",
      "After 200 batches, average loss/100 batches: 78.13387950897217\n",
      "After 300 batches, average loss/100 batches: 78.00589893341065\n",
      "After 400 batches, average loss/100 batches: 78.38679424285888\n",
      "After 500 batches, average loss/100 batches: 78.79577224731446\n",
      "After 600 batches, average loss/100 batches: 80.39667854309081\n",
      "After 700 batches, average loss/100 batches: 81.80359367370606\n",
      "After 800 batches, average loss/100 batches: 79.365510597229\n",
      "After 900 batches, average loss/100 batches: 78.23280651092529\n",
      "After 1000 batches, average loss/100 batches: 81.5290382194519\n",
      "After 1100 batches, average loss/100 batches: 80.5348287010193\n",
      "After 1200 batches, average loss/100 batches: 80.35432189941406\n",
      "After 1300 batches, average loss/100 batches: 78.77635093688964\n",
      "After 1400 batches, average loss/100 batches: 84.95904777526856\n",
      "After 1500 batches, average loss/100 batches: 77.13896686553954\n",
      "After 1600 batches, average loss/100 batches: 80.06474185943604\n",
      "After 1700 batches, average loss/100 batches: 77.07958908081055\n",
      "After 1800 batches, average loss/100 batches: 77.21272590637207\n",
      "After 1900 batches, average loss/100 batches: 78.32275924682617\n",
      "After 2000 batches, average loss/100 batches: 72.45319980621338\n",
      "After 2100 batches, average loss/100 batches: 77.33801845550538\n",
      "After 2200 batches, average loss/100 batches: 82.06023960113525\n",
      "After 2300 batches, average loss/100 batches: 82.79126922607422\n",
      "After 2400 batches, average loss/100 batches: 81.2564635848999\n",
      "Epoch 10/25\n",
      "After 100 batches, average loss/100 batches: 81.4112387084961\n",
      "After 200 batches, average loss/100 batches: 81.15312446594238\n",
      "After 300 batches, average loss/100 batches: 80.58228336334228\n",
      "After 400 batches, average loss/100 batches: 79.09631484985351\n",
      "After 500 batches, average loss/100 batches: 81.22863288879394\n",
      "After 600 batches, average loss/100 batches: 83.31730354309082\n",
      "After 700 batches, average loss/100 batches: 85.52951419830322\n",
      "After 800 batches, average loss/100 batches: 78.3642374420166\n",
      "After 900 batches, average loss/100 batches: 81.81305862426758\n",
      "After 1000 batches, average loss/100 batches: 77.46942890167236\n",
      "After 1100 batches, average loss/100 batches: 79.43169631958008\n",
      "After 1200 batches, average loss/100 batches: 78.31618560791016\n",
      "After 1300 batches, average loss/100 batches: 86.44259971618652\n",
      "After 1400 batches, average loss/100 batches: 81.21273345947266\n",
      "After 1500 batches, average loss/100 batches: 83.74620738983154\n",
      "After 1600 batches, average loss/100 batches: 77.57187065124512\n",
      "After 1700 batches, average loss/100 batches: 80.94091487884522\n",
      "After 1800 batches, average loss/100 batches: 81.08702098846436\n",
      "After 1900 batches, average loss/100 batches: 81.22792148590088\n",
      "After 2000 batches, average loss/100 batches: 78.25505550384521\n",
      "After 2100 batches, average loss/100 batches: 83.71469543457032\n",
      "After 2200 batches, average loss/100 batches: 79.17900344848633\n",
      "After 2300 batches, average loss/100 batches: 77.04173503875732\n",
      "After 2400 batches, average loss/100 batches: 78.76271656036377\n",
      "Epoch 11/25\n",
      "After 100 batches, average loss/100 batches: 81.03028087615967\n",
      "After 200 batches, average loss/100 batches: 78.80130420684814\n",
      "After 300 batches, average loss/100 batches: 79.18782897949218\n",
      "After 400 batches, average loss/100 batches: 81.69460361480714\n",
      "After 500 batches, average loss/100 batches: 85.9801400756836\n",
      "After 600 batches, average loss/100 batches: 77.30204906463624\n",
      "After 700 batches, average loss/100 batches: 81.31704154968261\n",
      "After 800 batches, average loss/100 batches: 79.80372737884521\n",
      "After 900 batches, average loss/100 batches: 80.31411838531494\n",
      "After 1000 batches, average loss/100 batches: 77.97094612121582\n",
      "After 1100 batches, average loss/100 batches: 85.03587451934814\n",
      "After 1200 batches, average loss/100 batches: 76.29656421661377\n",
      "After 1300 batches, average loss/100 batches: 82.17277278900147\n",
      "After 1400 batches, average loss/100 batches: 82.01540882110595\n",
      "After 1500 batches, average loss/100 batches: 78.98621313095093\n",
      "After 1600 batches, average loss/100 batches: 78.71617671966553\n",
      "After 1700 batches, average loss/100 batches: 77.37879230499267\n",
      "After 1800 batches, average loss/100 batches: 77.53163570404053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1900 batches, average loss/100 batches: 83.39104915618897\n",
      "After 2000 batches, average loss/100 batches: 76.655425491333\n",
      "After 2100 batches, average loss/100 batches: 85.46368213653564\n",
      "After 2200 batches, average loss/100 batches: 80.06365928649902\n",
      "After 2300 batches, average loss/100 batches: 77.5754443359375\n",
      "After 2400 batches, average loss/100 batches: 80.56542201995849\n",
      "Epoch 12/25\n",
      "After 100 batches, average loss/100 batches: 80.85513097763061\n",
      "After 200 batches, average loss/100 batches: 79.75637836456299\n",
      "After 300 batches, average loss/100 batches: 80.92876247406006\n",
      "After 400 batches, average loss/100 batches: 80.67231349945068\n",
      "After 500 batches, average loss/100 batches: 80.82845897674561\n",
      "After 600 batches, average loss/100 batches: 78.635363407135\n",
      "After 700 batches, average loss/100 batches: 74.73740753173828\n",
      "After 800 batches, average loss/100 batches: 81.39788818359375\n",
      "After 900 batches, average loss/100 batches: 76.17833911895752\n",
      "After 1000 batches, average loss/100 batches: 81.16441177368164\n",
      "After 1100 batches, average loss/100 batches: 78.12178974151611\n",
      "After 1200 batches, average loss/100 batches: 81.30416717529297\n",
      "After 1300 batches, average loss/100 batches: 82.73484600067138\n",
      "After 1400 batches, average loss/100 batches: 85.17440834045411\n",
      "After 1500 batches, average loss/100 batches: 82.73198238372802\n",
      "After 1600 batches, average loss/100 batches: 83.58573574066162\n",
      "After 1700 batches, average loss/100 batches: 78.13261371612549\n",
      "After 1800 batches, average loss/100 batches: 83.83592864990234\n",
      "After 1900 batches, average loss/100 batches: 77.6673104095459\n",
      "After 2000 batches, average loss/100 batches: 80.08536117553712\n",
      "After 2100 batches, average loss/100 batches: 79.83494152069092\n",
      "After 2200 batches, average loss/100 batches: 82.30794750213623\n",
      "After 2300 batches, average loss/100 batches: 82.14333000183106\n",
      "After 2400 batches, average loss/100 batches: 80.94350738525391\n",
      "Epoch 13/25\n",
      "After 100 batches, average loss/100 batches: 77.64438205718994\n",
      "After 200 batches, average loss/100 batches: 82.60941791534424\n",
      "After 300 batches, average loss/100 batches: 78.25060401916504\n",
      "After 400 batches, average loss/100 batches: 81.83035209655762\n",
      "After 500 batches, average loss/100 batches: 80.50521484375\n",
      "After 600 batches, average loss/100 batches: 76.61860286712647\n",
      "After 700 batches, average loss/100 batches: 74.88367858886718\n",
      "After 800 batches, average loss/100 batches: 81.98477615356445\n",
      "After 900 batches, average loss/100 batches: 80.58826023101807\n",
      "After 1000 batches, average loss/100 batches: 80.48303874969483\n",
      "After 1100 batches, average loss/100 batches: 77.23806770324707\n",
      "After 1200 batches, average loss/100 batches: 82.6734687423706\n",
      "After 1300 batches, average loss/100 batches: 80.70628223419189\n",
      "After 1400 batches, average loss/100 batches: 81.654566116333\n",
      "After 1500 batches, average loss/100 batches: 75.5545432472229\n",
      "After 1600 batches, average loss/100 batches: 73.33645942687988\n",
      "After 1700 batches, average loss/100 batches: 78.42159885406494\n",
      "After 1800 batches, average loss/100 batches: 78.43937423706055\n",
      "After 1900 batches, average loss/100 batches: 82.70347019195556\n",
      "After 2000 batches, average loss/100 batches: 79.03539798736573\n",
      "After 2100 batches, average loss/100 batches: 81.86918552398681\n",
      "After 2200 batches, average loss/100 batches: 80.54427013397216\n",
      "After 2300 batches, average loss/100 batches: 79.48360916137695\n",
      "After 2400 batches, average loss/100 batches: 80.35655616760253\n",
      "Epoch 14/25\n",
      "After 100 batches, average loss/100 batches: 82.18228527069091\n",
      "After 200 batches, average loss/100 batches: 78.09184127807617\n",
      "After 300 batches, average loss/100 batches: 82.88998126983643\n",
      "After 400 batches, average loss/100 batches: 78.12104705810548\n",
      "After 500 batches, average loss/100 batches: 76.12653762817382\n",
      "After 600 batches, average loss/100 batches: 79.98652488708495\n",
      "After 700 batches, average loss/100 batches: 81.04634075164795\n",
      "After 800 batches, average loss/100 batches: 80.76128067016602\n",
      "After 900 batches, average loss/100 batches: 75.07337345123291\n",
      "After 1000 batches, average loss/100 batches: 76.26977733612061\n",
      "After 1100 batches, average loss/100 batches: 79.06632987976074\n",
      "After 1200 batches, average loss/100 batches: 79.89642349243164\n",
      "After 1300 batches, average loss/100 batches: 80.14452831268311\n",
      "After 1400 batches, average loss/100 batches: 80.84536697387695\n",
      "After 1500 batches, average loss/100 batches: 75.18874835968018\n",
      "After 1600 batches, average loss/100 batches: 83.7248832321167\n",
      "After 1700 batches, average loss/100 batches: 73.31741085052491\n",
      "After 1800 batches, average loss/100 batches: 79.70070266723633\n",
      "After 1900 batches, average loss/100 batches: 81.65267513275147\n",
      "After 2000 batches, average loss/100 batches: 83.07910499572753\n",
      "After 2100 batches, average loss/100 batches: 78.85186241149903\n",
      "After 2200 batches, average loss/100 batches: 80.76784408569335\n",
      "After 2300 batches, average loss/100 batches: 79.87113529205322\n",
      "After 2400 batches, average loss/100 batches: 79.77241111755372\n",
      "Epoch 15/25\n",
      "After 100 batches, average loss/100 batches: 86.2271806716919\n",
      "After 200 batches, average loss/100 batches: 80.04559413909912\n",
      "After 300 batches, average loss/100 batches: 81.08332153320312\n",
      "After 400 batches, average loss/100 batches: 76.71192386627197\n",
      "After 500 batches, average loss/100 batches: 81.93864818572997\n",
      "After 600 batches, average loss/100 batches: 78.78029434204102\n",
      "After 700 batches, average loss/100 batches: 83.49135570526123\n",
      "After 800 batches, average loss/100 batches: 78.93067783355713\n",
      "After 900 batches, average loss/100 batches: 80.45472492218018\n",
      "After 1000 batches, average loss/100 batches: 84.74157737731933\n",
      "After 1100 batches, average loss/100 batches: 79.43568939208984\n",
      "After 1200 batches, average loss/100 batches: 83.6927212715149\n",
      "After 1300 batches, average loss/100 batches: 79.14091979980469\n",
      "After 1400 batches, average loss/100 batches: 83.51710109710693\n",
      "After 1500 batches, average loss/100 batches: 74.28478052139282\n",
      "After 1600 batches, average loss/100 batches: 79.9310319519043\n",
      "After 1700 batches, average loss/100 batches: 81.84530502319336\n",
      "After 1800 batches, average loss/100 batches: 79.02115013122558\n",
      "After 1900 batches, average loss/100 batches: 76.6232900238037\n",
      "After 2000 batches, average loss/100 batches: 81.932356300354\n",
      "After 2100 batches, average loss/100 batches: 78.8760131072998\n",
      "After 2200 batches, average loss/100 batches: 78.08106300354004\n",
      "After 2300 batches, average loss/100 batches: 78.20130878448487\n",
      "After 2400 batches, average loss/100 batches: 76.90404045104981\n",
      "Epoch 16/25\n",
      "After 100 batches, average loss/100 batches: 76.13897983551026\n",
      "After 200 batches, average loss/100 batches: 76.4351378250122\n",
      "After 300 batches, average loss/100 batches: 80.17239566802978\n",
      "After 400 batches, average loss/100 batches: 79.22795188903808\n",
      "After 500 batches, average loss/100 batches: 79.99858032226562\n",
      "After 600 batches, average loss/100 batches: 82.40132148742676\n",
      "After 700 batches, average loss/100 batches: 78.1506143951416\n",
      "After 800 batches, average loss/100 batches: 75.42916725158692\n",
      "After 900 batches, average loss/100 batches: 79.25923820495605\n",
      "After 1000 batches, average loss/100 batches: 82.31290718078613\n",
      "After 1100 batches, average loss/100 batches: 80.30919139862061\n",
      "After 1200 batches, average loss/100 batches: 77.6019882965088\n",
      "After 1300 batches, average loss/100 batches: 81.27563154220582\n",
      "After 1400 batches, average loss/100 batches: 78.16756618499755\n",
      "After 1500 batches, average loss/100 batches: 83.05610733032226\n",
      "After 1600 batches, average loss/100 batches: 76.58197448730469\n",
      "After 1700 batches, average loss/100 batches: 81.13921474456787\n",
      "After 1800 batches, average loss/100 batches: 82.80693382263183\n",
      "After 1900 batches, average loss/100 batches: 80.46508949279786\n",
      "After 2000 batches, average loss/100 batches: 78.9603217124939\n",
      "After 2100 batches, average loss/100 batches: 79.27691925048828\n",
      "After 2200 batches, average loss/100 batches: 75.9875452041626\n",
      "After 2300 batches, average loss/100 batches: 77.56190383911132\n",
      "After 2400 batches, average loss/100 batches: 74.58747962951661\n",
      "Epoch 17/25\n",
      "After 100 batches, average loss/100 batches: 78.19714431762695\n",
      "After 200 batches, average loss/100 batches: 75.70693141937255\n",
      "After 300 batches, average loss/100 batches: 82.91507495880127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 400 batches, average loss/100 batches: 84.92262863159179\n",
      "After 500 batches, average loss/100 batches: 80.91072757720947\n",
      "After 600 batches, average loss/100 batches: 78.08289093017578\n",
      "After 700 batches, average loss/100 batches: 83.0232364654541\n",
      "After 800 batches, average loss/100 batches: 75.88555599212647\n",
      "After 900 batches, average loss/100 batches: 81.12280452728271\n",
      "After 1000 batches, average loss/100 batches: 82.26556632995606\n",
      "After 1100 batches, average loss/100 batches: 75.49871444702148\n",
      "After 1200 batches, average loss/100 batches: 81.2570351409912\n",
      "After 1300 batches, average loss/100 batches: 76.50247680664063\n",
      "After 1400 batches, average loss/100 batches: 80.68050968170166\n",
      "After 1500 batches, average loss/100 batches: 78.02974227905274\n",
      "After 1600 batches, average loss/100 batches: 79.72847465515137\n",
      "After 1700 batches, average loss/100 batches: 78.14262397766113\n",
      "After 1800 batches, average loss/100 batches: 79.1760050201416\n",
      "After 1900 batches, average loss/100 batches: 82.30736560821533\n",
      "After 2000 batches, average loss/100 batches: 86.45983242034912\n",
      "After 2100 batches, average loss/100 batches: 76.8003158569336\n",
      "After 2200 batches, average loss/100 batches: 84.53817123413086\n",
      "After 2300 batches, average loss/100 batches: 81.07295303344726\n",
      "After 2400 batches, average loss/100 batches: 80.34792373657227\n",
      "Epoch 18/25\n",
      "After 100 batches, average loss/100 batches: 79.91902431488037\n",
      "After 200 batches, average loss/100 batches: 80.56115463256836\n",
      "After 300 batches, average loss/100 batches: 81.25566959381104\n",
      "After 400 batches, average loss/100 batches: 77.75964401245118\n",
      "After 500 batches, average loss/100 batches: 77.2693970489502\n",
      "After 600 batches, average loss/100 batches: 81.08660484313965\n",
      "After 700 batches, average loss/100 batches: 82.43775764465332\n",
      "After 800 batches, average loss/100 batches: 77.68521633148194\n",
      "After 900 batches, average loss/100 batches: 81.59652603149414\n",
      "After 1000 batches, average loss/100 batches: 76.36981822967529\n",
      "After 1100 batches, average loss/100 batches: 78.92030254364013\n",
      "After 1200 batches, average loss/100 batches: 82.71666662216187\n",
      "After 1300 batches, average loss/100 batches: 76.296123046875\n",
      "After 1400 batches, average loss/100 batches: 81.89922580718994\n",
      "After 1500 batches, average loss/100 batches: 77.08779335021973\n",
      "After 1600 batches, average loss/100 batches: 80.07278106689454\n",
      "After 1700 batches, average loss/100 batches: 78.52878452301026\n",
      "After 1800 batches, average loss/100 batches: 79.01185771942139\n",
      "After 1900 batches, average loss/100 batches: 75.72676982879639\n",
      "After 2000 batches, average loss/100 batches: 77.33946968078614\n",
      "After 2100 batches, average loss/100 batches: 75.30291870117188\n",
      "After 2200 batches, average loss/100 batches: 80.7576250076294\n",
      "After 2300 batches, average loss/100 batches: 77.34447105407715\n",
      "After 2400 batches, average loss/100 batches: 80.28605663299561\n",
      "Epoch 19/25\n",
      "After 100 batches, average loss/100 batches: 83.06684852600098\n",
      "After 200 batches, average loss/100 batches: 79.48821014404297\n",
      "After 300 batches, average loss/100 batches: 79.68199275970458\n",
      "After 400 batches, average loss/100 batches: 77.08464660644532\n",
      "After 500 batches, average loss/100 batches: 78.21618564605713\n",
      "After 600 batches, average loss/100 batches: 83.16785614013672\n",
      "After 700 batches, average loss/100 batches: 75.81283027648925\n",
      "After 800 batches, average loss/100 batches: 79.07740337371825\n",
      "After 900 batches, average loss/100 batches: 84.2842989730835\n",
      "After 1000 batches, average loss/100 batches: 79.3143521118164\n",
      "After 1100 batches, average loss/100 batches: 79.11158166885376\n",
      "After 1200 batches, average loss/100 batches: 79.56525281906129\n",
      "After 1300 batches, average loss/100 batches: 81.61642803192139\n",
      "After 1400 batches, average loss/100 batches: 80.65735706329346\n",
      "After 1500 batches, average loss/100 batches: 80.66400363922119\n",
      "After 1600 batches, average loss/100 batches: 78.14973945617676\n",
      "After 1700 batches, average loss/100 batches: 82.13909782409668\n",
      "After 1800 batches, average loss/100 batches: 78.07131965637207\n",
      "After 1900 batches, average loss/100 batches: 75.94180599212646\n",
      "After 2000 batches, average loss/100 batches: 79.95045040130616\n",
      "After 2100 batches, average loss/100 batches: 80.13976264953614\n",
      "After 2200 batches, average loss/100 batches: 79.82336807250977\n",
      "After 2300 batches, average loss/100 batches: 78.60906452178955\n",
      "After 2400 batches, average loss/100 batches: 82.12075447082519\n",
      "Epoch 20/25\n",
      "After 100 batches, average loss/100 batches: 82.10750358581544\n",
      "After 200 batches, average loss/100 batches: 80.014984664917\n",
      "After 300 batches, average loss/100 batches: 80.32823112487793\n",
      "After 400 batches, average loss/100 batches: 79.36751831054687\n",
      "After 500 batches, average loss/100 batches: 78.09281826019287\n",
      "After 600 batches, average loss/100 batches: 79.86945751190186\n",
      "After 700 batches, average loss/100 batches: 73.76492404937744\n",
      "After 800 batches, average loss/100 batches: 79.10735252380371\n",
      "After 900 batches, average loss/100 batches: 79.50922409057617\n",
      "After 1000 batches, average loss/100 batches: 81.1257606124878\n",
      "After 1100 batches, average loss/100 batches: 81.26264457702636\n",
      "After 1200 batches, average loss/100 batches: 80.95821598052979\n",
      "After 1300 batches, average loss/100 batches: 79.734891166687\n",
      "After 1400 batches, average loss/100 batches: 82.98983333587647\n",
      "After 1500 batches, average loss/100 batches: 80.34019653320313\n",
      "After 1600 batches, average loss/100 batches: 80.38459266662598\n",
      "After 1700 batches, average loss/100 batches: 82.73221557617188\n",
      "After 1800 batches, average loss/100 batches: 81.71084251403809\n",
      "After 1900 batches, average loss/100 batches: 80.83786373138427\n",
      "After 2000 batches, average loss/100 batches: 79.53622802734375\n",
      "After 2100 batches, average loss/100 batches: 82.6120751953125\n",
      "After 2200 batches, average loss/100 batches: 80.59098709106445\n",
      "After 2300 batches, average loss/100 batches: 75.54303749084472\n",
      "After 2400 batches, average loss/100 batches: 80.13499534606933\n",
      "Epoch 21/25\n",
      "After 100 batches, average loss/100 batches: 79.80885704040527\n",
      "After 200 batches, average loss/100 batches: 80.57418632507324\n",
      "After 300 batches, average loss/100 batches: 81.9794612121582\n",
      "After 400 batches, average loss/100 batches: 75.04352096557618\n",
      "After 500 batches, average loss/100 batches: 77.32191730499268\n",
      "After 600 batches, average loss/100 batches: 78.32733955383301\n",
      "After 700 batches, average loss/100 batches: 75.99954496383667\n",
      "After 800 batches, average loss/100 batches: 79.72030002593993\n",
      "After 900 batches, average loss/100 batches: 76.56223781585693\n",
      "After 1000 batches, average loss/100 batches: 81.98331851959229\n",
      "After 1100 batches, average loss/100 batches: 76.56033882141114\n",
      "After 1200 batches, average loss/100 batches: 80.61318199157715\n",
      "After 1300 batches, average loss/100 batches: 85.01777122497559\n",
      "After 1400 batches, average loss/100 batches: 80.32554103851318\n",
      "After 1500 batches, average loss/100 batches: 77.0158092880249\n",
      "After 1600 batches, average loss/100 batches: 79.1562557220459\n",
      "After 1700 batches, average loss/100 batches: 81.7124161529541\n",
      "After 1800 batches, average loss/100 batches: 80.08158302307129\n",
      "After 1900 batches, average loss/100 batches: 78.86346046447754\n",
      "After 2000 batches, average loss/100 batches: 77.3653350830078\n",
      "After 2100 batches, average loss/100 batches: 80.91562747955322\n",
      "After 2200 batches, average loss/100 batches: 83.63329444885254\n",
      "After 2300 batches, average loss/100 batches: 75.44159114837646\n",
      "After 2400 batches, average loss/100 batches: 77.54486545562744\n",
      "Epoch 22/25\n",
      "After 100 batches, average loss/100 batches: 79.24216781616211\n",
      "After 200 batches, average loss/100 batches: 75.20795978546143\n",
      "After 300 batches, average loss/100 batches: 78.62883613586426\n",
      "After 400 batches, average loss/100 batches: 75.96874473571778\n",
      "After 500 batches, average loss/100 batches: 81.89390449523925\n",
      "After 600 batches, average loss/100 batches: 79.11625991821289\n",
      "After 700 batches, average loss/100 batches: 79.14232692718505\n",
      "After 800 batches, average loss/100 batches: 79.00905128479003\n",
      "After 900 batches, average loss/100 batches: 80.92790023803711\n",
      "After 1000 batches, average loss/100 batches: 78.86034545898437\n",
      "After 1100 batches, average loss/100 batches: 79.63160255432129\n",
      "After 1200 batches, average loss/100 batches: 79.92296440124511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1300 batches, average loss/100 batches: 80.07353569030762\n",
      "After 1400 batches, average loss/100 batches: 74.77135082244872\n",
      "After 1500 batches, average loss/100 batches: 82.13839933395386\n",
      "After 1600 batches, average loss/100 batches: 77.42219871520996\n",
      "After 1700 batches, average loss/100 batches: 80.92265396118164\n",
      "After 1800 batches, average loss/100 batches: 77.8900549697876\n",
      "After 1900 batches, average loss/100 batches: 79.22735229492187\n",
      "After 2000 batches, average loss/100 batches: 79.45392837524415\n",
      "After 2100 batches, average loss/100 batches: 77.9352583694458\n",
      "After 2200 batches, average loss/100 batches: 75.49488655090332\n",
      "After 2300 batches, average loss/100 batches: 80.49289291381837\n",
      "After 2400 batches, average loss/100 batches: 78.91890731811523\n",
      "Epoch 23/25\n",
      "After 100 batches, average loss/100 batches: 80.23210235595702\n",
      "After 200 batches, average loss/100 batches: 77.25273990631104\n",
      "After 300 batches, average loss/100 batches: 81.64512329101562\n",
      "After 400 batches, average loss/100 batches: 77.6497472000122\n",
      "After 500 batches, average loss/100 batches: 78.83700416564942\n",
      "After 600 batches, average loss/100 batches: 75.17402847290039\n",
      "After 700 batches, average loss/100 batches: 82.088791847229\n",
      "After 800 batches, average loss/100 batches: 79.21073486328125\n",
      "After 900 batches, average loss/100 batches: 76.80733455657959\n",
      "After 1000 batches, average loss/100 batches: 81.11223136901856\n",
      "After 1100 batches, average loss/100 batches: 79.6326919555664\n",
      "After 1200 batches, average loss/100 batches: 79.71761318206786\n",
      "After 1300 batches, average loss/100 batches: 80.13454898834229\n",
      "After 1400 batches, average loss/100 batches: 79.87485034942627\n",
      "After 1500 batches, average loss/100 batches: 80.53332710266113\n",
      "After 1600 batches, average loss/100 batches: 79.75472881317138\n",
      "After 1700 batches, average loss/100 batches: 84.32580837249756\n",
      "After 1800 batches, average loss/100 batches: 77.9750212097168\n",
      "After 1900 batches, average loss/100 batches: 72.82085056304932\n",
      "After 2000 batches, average loss/100 batches: 77.16770029067993\n",
      "After 2100 batches, average loss/100 batches: 80.7342174911499\n",
      "After 2200 batches, average loss/100 batches: 75.97201316833497\n",
      "After 2300 batches, average loss/100 batches: 77.16608219146728\n",
      "After 2400 batches, average loss/100 batches: 75.75105319976807\n",
      "Epoch 24/25\n",
      "After 100 batches, average loss/100 batches: 80.20128925323486\n",
      "After 200 batches, average loss/100 batches: 80.08047481536865\n",
      "After 300 batches, average loss/100 batches: 76.52050052642822\n",
      "After 400 batches, average loss/100 batches: 78.39152034759522\n",
      "After 500 batches, average loss/100 batches: 81.10070373535156\n",
      "After 600 batches, average loss/100 batches: 82.41381126403809\n",
      "After 700 batches, average loss/100 batches: 77.3692338180542\n",
      "After 800 batches, average loss/100 batches: 77.48247234344483\n",
      "After 900 batches, average loss/100 batches: 79.51676586151123\n",
      "After 1000 batches, average loss/100 batches: 79.46365730285645\n",
      "After 1100 batches, average loss/100 batches: 79.2885612487793\n",
      "After 1200 batches, average loss/100 batches: 81.87098636627198\n",
      "After 1300 batches, average loss/100 batches: 79.66066055297851\n",
      "After 1400 batches, average loss/100 batches: 79.50600509643554\n",
      "After 1500 batches, average loss/100 batches: 77.13324764251709\n",
      "After 1600 batches, average loss/100 batches: 77.7836247253418\n",
      "After 1700 batches, average loss/100 batches: 80.44836505889893\n",
      "After 1800 batches, average loss/100 batches: 81.40447135925292\n",
      "After 1900 batches, average loss/100 batches: 78.04230617523193\n",
      "After 2000 batches, average loss/100 batches: 78.99009971618652\n",
      "After 2100 batches, average loss/100 batches: 81.6550849533081\n",
      "After 2200 batches, average loss/100 batches: 78.26507289886474\n",
      "After 2300 batches, average loss/100 batches: 72.16590335845947\n",
      "After 2400 batches, average loss/100 batches: 82.00545806884766\n",
      "Epoch 25/25\n",
      "After 100 batches, average loss/100 batches: 81.55213630676269\n",
      "After 200 batches, average loss/100 batches: 77.73350814819337\n",
      "After 300 batches, average loss/100 batches: 85.47186164855957\n",
      "After 400 batches, average loss/100 batches: 79.26296157836914\n",
      "After 500 batches, average loss/100 batches: 77.71078285217285\n",
      "After 600 batches, average loss/100 batches: 77.55410884857177\n",
      "After 700 batches, average loss/100 batches: 81.29078186035156\n",
      "After 800 batches, average loss/100 batches: 79.5634264755249\n",
      "After 900 batches, average loss/100 batches: 73.7941337966919\n",
      "After 1000 batches, average loss/100 batches: 80.19805164337158\n",
      "After 1100 batches, average loss/100 batches: 75.61254566192628\n",
      "After 1200 batches, average loss/100 batches: 80.86897075653076\n",
      "After 1300 batches, average loss/100 batches: 75.51381801605224\n",
      "After 1400 batches, average loss/100 batches: 75.76919282913208\n",
      "After 1500 batches, average loss/100 batches: 79.45283203125\n",
      "After 1600 batches, average loss/100 batches: 78.57395141601563\n",
      "After 1700 batches, average loss/100 batches: 77.77909088134766\n",
      "After 1800 batches, average loss/100 batches: 80.60302597045899\n",
      "After 1900 batches, average loss/100 batches: 78.31691223144531\n",
      "After 2000 batches, average loss/100 batches: 77.63746871948243\n",
      "After 2100 batches, average loss/100 batches: 79.18675640106201\n",
      "After 2200 batches, average loss/100 batches: 77.61306098937989\n",
      "After 2300 batches, average loss/100 batches: 79.13557147979736\n",
      "After 2400 batches, average loss/100 batches: 81.91558887481689\n"
     ]
    }
   ],
   "source": [
    "learning_rate = learning_rate / 10\n",
    "gru_losses = trainIters(encoder_gru, decoder_gru, dataloader, epochs=25, learning_rate = learning_rate)\n",
    "np.save('gru2_losses.npy', gru_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder_gru.state_dict(), 'encoder2_gru.pth')\n",
    "torch.save(decoder_gru.state_dict(), 'decoder2_gru.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataloader):\n",
    "    for batch in dataloader:\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_tensor, encoder, decoder):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden = encoder.initHidden(1)\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor.to(device), encoder_hidden)\n",
    "\n",
    "        decoder_input =  torch.tensor([word2index[START]]*input_tensor.shape[0], dtype=torch.long, device=device).unsqueeze(0)\n",
    "        try:\n",
    "            encoder.lstm\n",
    "            decoder_hidden = (encoder_hidden[0][1::2].contiguous(), encoder_hidden[1][1::2].contiguous())\n",
    "        except AttributeError:\n",
    "            decoder_hidden = encoder_hidden[1::2].contiguous()\n",
    "\n",
    "        output_list = []\n",
    "        attn_weight_list = np.zeros((max_len, max_len))\n",
    "        for di in range(1, max_len):\n",
    "            output, decoder_hidden, attn_weights = decoder(decoder_input,\n",
    "                                                           decoder_hidden,\n",
    "                                                           encoder_output)\n",
    "\n",
    "            decoder_input = output.topk(1)[1].detach()\n",
    "            output_list.append(output.topk(1)[1])\n",
    "            word = index2word[output.topk(1)[1].item()]\n",
    "\n",
    "            attn_weight_list[di] += attn_weights[0,0,:].cpu().numpy()\n",
    "        return output_list, attn_weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch(dataloader)\n",
    "input_tensor = batch['input_tensor'][11].unsqueeze_(0)\n",
    "gru_output_list, gru_attn = evaluate(input_tensor, encoder_gru, decoder_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input and Output: \n",
      "<s> sheldonspeaks hello , there pennyspeaks </s>\n",
      " <s> pennyspeaks what are you doing ? </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predictScript(input):\n",
    "    doc = nlp(input)\n",
    "    input_script = [START] + [word.norm_.lower().strip() if word.norm_ != 'gonna' else 'a' for word in doc ] + [END]\n",
    "    inp_tensor = torch.zeros(max_len, dtype=torch.long)\n",
    "    for i, word in enumerate(input_script):\n",
    "        if word in word2index and word_count[word] > 5:\n",
    "            inp_tensor[i] = word2index[word]\n",
    "        else:\n",
    "            print(\"Missing Word: \", word)\n",
    "    gru_output_list, _ = evaluate(inp_tensor.unsqueeze_(0), encoder_gru, decoder_gru)\n",
    "    output = \" \".join(input_script) + \"\\n\"\n",
    "    print(\"Input and Output: \")\n",
    "    for index in gru_output_list:\n",
    "        word = index2word[index[0,0].item()]\n",
    "        if word != '</s>':\n",
    "            output += ' ' + word\n",
    "        else:\n",
    "            output += ' ' + word \n",
    "            print(output.strip(), end = \"\\n\\n\")\n",
    "            break\n",
    "    \n",
    "\n",
    "predictScript()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input and Output: \n",
      "<s> sheldonspeaks hello ! pennyspeaks </s>\n",
      " pennyspeaks hey , you , </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> sheldonspeaks can you drive me to work ? pennyspeaks </s>\n",
      " pennyspeaks no . </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> sheldonspeaks knock knock pennyspeaks </s>\n",
      " pennyspeaks oh , </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> sheldonspeaks i want ice cream . pennyspeaks </s>\n",
      " pennyspeaks yeah , i . </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> leonardspeaks where is amy ? . sheldonspeaks </s>\n",
      " sheldonspeaks it 's <unk> . </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> howardspeaks shall we play paint ball ? sheldonspeaks </s>\n",
      " sheldonspeaks we are <unk> . . . . . . . . . </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scripts = [\"sheldonspeaks hello! pennyspeaks\", \n",
    "           \"sheldonspeaks Can you drive me to work? pennyspeaks\", \n",
    "           \"sheldonspeaks knock knock pennyspeaks\", \n",
    "           \"sheldonspeaks i want ice cream. pennyspeaks\",\n",
    "           \"leonardspeaks where is amy?. sheldonspeaks\",\n",
    "           \"howardspeaks shall we play paint ball? sheldonspeaks\"\n",
    "        ]\n",
    "\n",
    "for script in scripts:\n",
    "    predictScript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input and Output: \n",
      "<s> sheldonspeaks hello ! pennyspeaks </s>\n",
      " pennyspeaks hey , you , </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> sheldonspeaks can you drive me to work ? pennyspeaks </s>\n",
      " pennyspeaks no . </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> sheldonspeaks knock knock pennyspeaks </s>\n",
      " pennyspeaks oh , </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> sheldonspeaks i want ice cream . pennyspeaks </s>\n",
      " pennyspeaks yeah , i , . . </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> leonardspeaks where is amy ? . sheldonspeaks </s>\n",
      " sheldonspeaks <unk> . . </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> howardspeaks shall we play paint ball ? sheldonspeaks </s>\n",
      " sheldonspeaks i . . . . . . . . . </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scripts = [\"sheldonspeaks hello! pennyspeaks\", \n",
    "           \"sheldonspeaks Can you drive me to work? pennyspeaks\", \n",
    "           \"sheldonspeaks knock knock pennyspeaks\", \n",
    "           \"sheldonspeaks i want ice cream. pennyspeaks\",\n",
    "           \"leonardspeaks where is amy?. sheldonspeaks\",\n",
    "           \"howardspeaks shall we play paint ball? sheldonspeaks\"\n",
    "        ]\n",
    "\n",
    "for script in scripts:\n",
    "    predictScript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input and Output: \n",
      "<s> sheldonspeaks hello ! pennyspeaks </s>\n",
      " <s> pennyspeaks hi . </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> sheldonspeaks can you drive me to work ? pennyspeaks </s>\n",
      " <s> pennyspeaks no . </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> sheldonspeaks knock knock pennyspeaks </s>\n",
      " <s> pennyspeaks what are you doing here ? </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> sheldonspeaks i want ice cream . pennyspeaks </s>\n",
      " <s> pennyspeaks oh - huh . </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> leonardspeaks where is amy ? . sheldonspeaks </s>\n",
      " <s> sheldonspeaks in this room , leonard and i are ready to get rid of you like pizza and a half . </s>\n",
      "\n",
      "Input and Output: \n",
      "<s> howardspeaks shall we play paint ball ? sheldonspeaks </s>\n",
      " <s> sheldonspeaks in a <unk> </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scripts = [\"sheldonspeaks hello! pennyspeaks\", \n",
    "           \"sheldonspeaks Can you drive me to work? pennyspeaks\", \n",
    "           \"sheldonspeaks knock knock pennyspeaks\", \n",
    "           \"sheldonspeaks i want ice cream. pennyspeaks\",\n",
    "           \"leonardspeaks where is amy?. sheldonspeaks\",\n",
    "           \"howardspeaks shall we play paint ball? sheldonspeaks\"\n",
    "        ]\n",
    "\n",
    "for script in scripts:\n",
    "    predictScript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scenespeaks The hallway. Leonard knocks on Penny’s door.',\n",
       " 'Scenespeaks Sheldon’s bedroom. He is building a model of some kind of double helix. There is a knock on the door.',\n",
       " 'Scenespeaks The apartment living room. There is a knock on the door.',\n",
       " 'Scenespeaks The same, later. Leonard is dressed as Frodo. Howard appears to be Peter Pan. There is a knock on the door.',\n",
       " 'Scenespeaks The hallway. Howard knocks on Penny’s door with his bow.',\n",
       " 'Scenespeaks The apartment, there is a knock on the door.',\n",
       " 'Scenespeaks Outside Penny’s door. Leonard knocks.',\n",
       " 'Scenespeaks Sheldon’s office. He is making measurements on maps. There is a knock on the door.',\n",
       " 'Howardspeaks And even if you can make it to Boston, what are you going to do, knock on the door and say to Mrs Bell, “hey Mrs Bell, big fan of your husband, can I come in and watch him invent the telephone?”',\n",
       " 'Rajspeaks Mrs Bell was deaf, she’s not even going to hear you knock.',\n",
       " 'Sheldonspeaks Same paradox. If you were to travel back in time and, say, knock me unconscious, you would not then have the conversation that irritated you, motivating you to go back and knock me unconscious.',\n",
       " 'Leonardspeaks What if I knocked you unconscious right now?',\n",
       " 'Scenespeaks Howard and Raj sneak up the stairwell carrying presents. Howard knocks on Penny’s door, a combination of two knocks, two knocks, one knock. Nothing happens. He tries again. Sheldon opens the door.',\n",
       " 'Sheldonspeaks The whole point of a secret knock is to establish a non-verbal signal to verify the identity of one’s co-conspirators.',\n",
       " 'Scenespeaks Leonard approaches Penny’s door and knocks.',\n",
       " 'Scenespeaks Leonard approaches Penny’s door. He is wearing a suit. He knocks. Penny answers.',\n",
       " 'Scenespeaks Sheldon’s bedroom. There is a knock on the door. Penny enters.',\n",
       " 'Pennyspeaks Come on, this is huge, Raj is going to be in People magazine. And he didn’t even have to knock up one of the Spears sisters.',\n",
       " 'Scenespeaks Outside Penny’s door. Raj in a suit knocks on the door with his foot as he has a glass of champagne in both hands. He drinks one. Penny opens door, she is dressed up also.',\n",
       " 'Scenespeaks The apartment. There is a knock on the door which then opens and Penny enters.',\n",
       " 'Pennyspeaks Sheldon, honey, I’ve told you, it’s a small apartment, you only have to knock one time.',\n",
       " 'Sheldonspeaks (Knock, knock, knock) Penny, (knock, knock, knock) Penny, (knock, knock, knock) Penny… (door opens) I am very, very sorry for what I have done. Here’s your laundry, I rescind your strikes and you are no longer banished.',\n",
       " 'Scenespeaks Outside Penny’s apartment. Sheldon has his laptop. He knocks three times, the presses a button and an electronic voice says “Penny”. He does this three times.',\n",
       " 'Scenespeaks Outside the apartment. Penny knocks.',\n",
       " 'Sheldonspeaks So, Penny has a don’t knock on my door before eleven o’clock\\xa0 or I punch you in the throat rule.',\n",
       " 'Scenespeaks Outside Penny’s door. Sheldon stands looking at his watch with his hand poised to knock. At the right moment he starts knocking.',\n",
       " 'Scenespeaks The apartment. Sheldon is writing on an imaginary board. There is a knock on the door.',\n",
       " 'Sheldonspeaks Well, I have and it will knock your socks off! Good luck getting them back on.',\n",
       " 'Scenespeaks Outside Penny’s door. Leonard knocks, Penny answers in her dressing gown.',\n",
       " 'Scenespeaks Sheldon’s bedroom. Sheldon is asleep. There is a knock on the door.',\n",
       " 'Scenespeaks Outside Penny’s door. Leonard is knocking.',\n",
       " 'Scenespeaks Sheldon’s bedroom. Penny knocks and enters.',\n",
       " 'Scenespeaks Raj’s apartment. Raj is watching a Bollywood movie. There is a knock on the door.',\n",
       " 'Rajspeaks Stop knocking! It’s open! Please tell my parents that our dark matter research is at a critical juncture, and I can’t come home for my cousin Sanjay’s wedding.',\n",
       " 'Pennyspeaks Get in here! Hurry! Don’t you dare knock.',\n",
       " 'Howardspeaks So two years later, there’s a knock on the door, guy opens it, and there on his porch is the snail, who says, “What the heck was all that about?”',\n",
       " 'Leonardspeaks It was us. We knocked over a lamp.',\n",
       " 'Sheldonspeaks Why would you knock over a lamp?',\n",
       " 'Scenespeaks Leonard’s bedroom. Sheldon is standing over their bed. He knocks on the wall.',\n",
       " 'Scenespeaks Leonard’s bedroom. There is a knock on the door.',\n",
       " 'Pennyspeaks You do realize I stand on the other side of the door waiting for you to finish knocking three times.',\n",
       " 'Sheldonspeaks So this conversation is as pointless as your door-knocking soliloquy?',\n",
       " 'Scenespeaks The apartment. There is a knock on the door.',\n",
       " 'Bernadettespeaks Knock-knock.',\n",
       " 'Howardspeaks Knock-knock.',\n",
       " 'Sheldonspeaks Knock-knock.',\n",
       " 'Sheldonspeaks Hugh people need to listen to me. It’s time to get in line for the movie. And that’s how you tell a knock-knock joke.',\n",
       " 'Bernadettespeaks Knock-knock.',\n",
       " 'Scenespeaks At Penny’s door. Leonard knocks three times.',\n",
       " 'Sheldonspeaks I’m the Flash. I just knocked 30,000 times.',\n",
       " 'Pennyspeaks Okay, these are Uggs. These are Crocs. These are knockoff Manolo Blahniks.',\n",
       " 'Scenespeaks Raj’s Apartment. Leonard knocks on the door.',\n",
       " 'Scenespeaks Bernadette’s apartment. Howard knocks on door.',\n",
       " 'Leonardspeaks Fine. You walk up to the house, knock on the door and demand your stuff back. What if he says no?',\n",
       " 'Howardspeaks So I knocked down the bathroom door, picked up my poor unconscious mother, carried her to the car, and drove like a madman to the emergency room.',\n",
       " 'Scenespeaks Penny’s apartment. Penny has a bottle of wine. She can’t find a glass. Finds a measuring jug and uses that instead. There is a knock on the door.',\n",
       " 'Scenespeaks Raj’s apartment. There is a knock on the door. He opens it.',\n",
       " 'Amyspeaks Oh… You are aware that your ritualistic knocking behaviour is symptomatic of obsessive compulsive disorder?',\n",
       " 'Scenespeaks Penny’s apartment. There is a knock.',\n",
       " 'Scenespeaks Penny’s apartment door. Sheldon knocks three times.',\n",
       " 'Leonardspeaks Uh, let’s see. Uh, I am an experimental physicist at Cal-Tech, most of my research is with high-powered lasers, and, oh, I’ve just gotten a big government grant to see if they can be used to knock out incoming ballistic missiles.',\n",
       " 'Scenespeaks Penny’s apartment door. Leonard knocks.',\n",
       " 'Leonardspeaks Since when don’t you knock? It’s like the only good thing about you.',\n",
       " 'Scenespeaks Howard’s hotel room. There is a knock on the door.',\n",
       " 'Sheldonspeaks Oh, that’s very kind of you. Next time I have a hankering to wash down a D cell battery with a jar of old pickle juice, I’ll come a-knocking.',\n",
       " 'Sheldonspeaks Knock, knock.',\n",
       " 'Rajspeaks Listen, I love your charming racist humour, but any chance you could not knock my religion while she’s here.',\n",
       " 'Scenespeaks Penny’s door. Howard knocks.',\n",
       " 'Rajspeaks You should go to my girl. She’ll knock out those sideburns for free.',\n",
       " 'Scenespeaks Penny’s bedroom. Sheldon is standing over Penny’s bed, knocking on the wall.',\n",
       " 'Sheldonspeaks Yeah, well, I knocked on the front door, but you didn’t hear it.',\n",
       " 'Sheldonspeaks (Knock, knock, knock) Human Resources Department. (Knock, knock, knock) Human Resources Department. (Knock, knock, knock) Human Resources Department.',\n",
       " 'Sheldonspeaks Now, see, I never would have thought to do that. Clearly, I made a good choice farming this out to you. But I am telling you, Amy hit the boyfriend jackpot. Anyway, my socks are on. Let’s knock them off.',\n",
       " 'Pennyspeaks You need me to shut the door so you can do your knocking thing?',\n",
       " 'Scenespeaks Outside Raj’s apartment. The girl from the comic book store knocks on the door.',\n",
       " 'Sheldonspeaks I’ve never knocked on my own door before. That was a wild ride.',\n",
       " 'Howardspeaks Yeah, you’re gonna knock it out of the park.',\n",
       " 'Amyspeaks It’s exhausting. Do you have any idea how hard it is to laugh at a knock-knock joke that starts with knock-knock-knock, Amy, knock-knock-knock, Amy, knock-knock-knock, Amy?',\n",
       " 'Rajspeaks Yeah, knock it off, Howard.',\n",
       " 'Pennyspeaks Oh, come on, it wasn’t me. Anyone could have knocked your mirror off, or whatever happened.',\n",
       " 'Scenespeaks Sheldon’s bedroom. There is a knock on the door.',\n",
       " 'Sheldonspeaks Oh, of course it’s no big deal to you. You idolize me, and nothing could ever knock me off that pedestal you put me on.',\n",
       " 'Pennyspeaks You knocked more than usual.',\n",
       " 'Howardspeaks Knock, knock.',\n",
       " 'Rajspeaks You know, if you knocked out this wall, it would give you an open floor plan, and then, it’s a little scary, but could be fun, indoor fire pit.',\n",
       " 'Howardspeaks Hey, I grew up in this house, okay? No one’s knocking anything down.',\n",
       " 'Sheldonspeaks I sure did. Oh, my goodness. Well, from Jabba’s head to ice cream with Darth Vader, I’m having a heck of a ride. Yeah, look, clearly, good things happen when I’m in charge. Now, why don’t you boys step aside, let me knock this project out?',\n",
       " 'MrsCooperspeaks Well, do it some more. Maybe you can knock some sense into yourself.',\n",
       " 'Scenespeaks Amy’s apartment. Sheldon knocks three times.',\n",
       " 'Amyspeaks And you didn’t do your compulsive knocking ritual so I would open the door.',\n",
       " 'Leonardspeaks Will you knock it off? We’re across the hall.',\n",
       " 'Susanspeaks Well, fine, if everyone wants to make jokes about our problems, then I can, too. Knock, knock. Who’s there? Our family is an embarrassment.',\n",
       " 'Bernadettespeaks Love is patient, but it’s not gonna put up with all the side chatter, so let’s knock it off.',\n",
       " 'Sheldonspeaks Interesting. If my official residence were across the hall, I wonder if I’d need to knock every time I came over here.',\n",
       " 'Sheldonspeaks Good morning. See? I didn’t knock, but it’s fine. I didn’t knock, but it’s fine. I didn’t knock, but it’s fine. So, how is everyone?',\n",
       " 'Sheldonspeaks I know. It’s also why I never open a door without knocking three times. I mean, the first',\n",
       " 'Sheldonspeaks Oh, don’t be. You get your hopes up, I knock them down. That’s called teamwork.',\n",
       " 'Pennyspeaks This is getting old fast, Dolores, knock it off.',\n",
       " 'Scenespeaks Outside Amy’s lab. Howard knocks on the door.',\n",
       " 'Pennyspeaks So, all we need to do is get Sheldon knocked up.',\n",
       " 'Sheldonspeaks Oh, hey. If you knocked, I couldn’t hear you. I’m welding this locomotive engine. And if you didn’t knock, how about some manners?',\n",
       " 'Sheldonspeaks Yeah, well, get this, neither does Penny, that’s why she doesn’t want to go. You set ’em up, I knock ’em down, good job.',\n",
       " 'Howardspeaks I told her if day care is anything like prison, find the biggest baby and knock him out.']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[script for script in modified_scripts if bool(re.search(\"knock\", script))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Script:\n",
      " <s> leonardspeaks i am sure she will still love him . sheldonspeaks </s>\n",
      "Model Prediction:\n",
      " <s> sheldonspeaks well , i do not <unk> . . . . </s>\n",
      "Output Script:\n",
      " <s> sheldonspeaks i would not . </s>\n"
     ]
    }
   ],
   "source": [
    "print('Input Script:')\n",
    "output = ''\n",
    "for index in input_tensor[0]:\n",
    "    word = index2word[index.item()]\n",
    "    if word != '</s>':\n",
    "        output += ' ' + word\n",
    "    else:\n",
    "        output += ' ' + word\n",
    "        print(output)\n",
    "        break\n",
    "        \n",
    "print('Model Prediction:')\n",
    "output = ''\n",
    "for index in gru_output_list:\n",
    "    word = index2word[index[0,0].item()]\n",
    "    if word != '</s>':\n",
    "        output += ' ' + word\n",
    "    else:\n",
    "        output += ' ' + word\n",
    "        print(output)\n",
    "        break\n",
    "        \n",
    "print('Output Script:')\n",
    "output = ''\n",
    "for index in batch['output_tensor'][11].unsqueeze_(0)[0]:\n",
    "    word = index2word[index.item()]\n",
    "    if word != '</s>':\n",
    "        output += ' ' + word\n",
    "    else:\n",
    "        output += ' ' + word\n",
    "        print(output)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
